{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import random\n",
    "from time import time\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(filename, banned_subreddits, max_comments=100000, print_freq=-1):\n",
    "    comments = []\n",
    "    \n",
    "    with bz2.BZ2File(filename, \"r\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if print_freq > 0 and i%print_freq ==0:\n",
    "                print(i, flush=True)\n",
    "            if i >= max_comments:\n",
    "                break\n",
    "            i += 1\n",
    "            \n",
    "            comm = json.loads(line.decode())\n",
    "            subreddit = comm[\"subreddit\"]\n",
    "            trimmed_comm = {\"subreddit\": subreddit,\n",
    "                            \"banned\": subreddit in banned_subreddits,\n",
    "                            \"body\": comm[\"body\"]}\n",
    "            comments.append(trimmed_comm)\n",
    "            \n",
    "    return comments\n",
    "\n",
    "\n",
    "\n",
    "def read_banned_subreddits(filename):\n",
    "    \"\"\"read in banned subreddits file with each line as 'r/asfasdf' \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        banned = set()\n",
    "        for line in f:\n",
    "            if line.strip() != \"\":\n",
    "                banned.update([line.strip().split(\"r/\")[1]])\n",
    "        return banned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def divide_chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chunkify_test(test, words_per_example=200):\n",
    "    subreddit_to_comments = dict()\n",
    "    for comm in test:\n",
    "        if comm[\"subreddit\"] in subreddit_to_comments:\n",
    "            subreddit_to_comments[comm[\"subreddit\"]].append(comm)\n",
    "        else:\n",
    "            subreddit_to_comments[comm[\"subreddit\"]] = [comm]\n",
    "\n",
    "    examples = flatten([chunkify_subreddit(subreddit, words_per_example)\n",
    "                        for _, subreddit in subreddit_to_comments.items()])\n",
    "    return examples\n",
    "\n",
    "    \n",
    "\n",
    "def split_data(subreddit_to_comments, ratio, words_per_example=200):\n",
    "    random.seed(42)\n",
    "\n",
    "    flattened = flatten([comments for _, comments in subreddit_to_comments.items()])\n",
    "    random.shuffle(flattened)\n",
    "\n",
    "    split_idx = int(len(flattened) * ratio)\n",
    "    \n",
    "    train = flattened[:split_idx]\n",
    "    test = flattened[split_idx:]\n",
    "\n",
    "    train_banned, train_notbanned = chunkify_train(train, words_per_example)\n",
    "    test_examples = chunkify_test(test, words_per_example)\n",
    "    return train_banned, train_notbanned, test_examples\n",
    "    \n",
    "\n",
    "def write_train_examples(examples, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"words\"])\n",
    "        for example in examples:\n",
    "            writer.writerow([\" \".join(example)])\n",
    "\n",
    "\n",
    "def write_test_examples(examples, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"subreddit\", \"banned\", \"words\"])\n",
    "        for example in examples:\n",
    "            writer.writerow([example[\"subreddit\"], example[\"banned\"], \" \".join(example[\"words\"])])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_block(comments, words_per_example=200, ratio=0.8):\n",
    "#     random.shuffle(comments)\n",
    "#     split_idx = int(len(comments) * ratio)\n",
    "    \n",
    "#     train = comments[:split_idx]\n",
    "#     test = comments[split_idx:]\n",
    "    \n",
    "#     train_banned, train_notbanned = chunkify_train(train, words_per_example)\n",
    "#     \n",
    "#     return train_banned, train_notbanned, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify_train(comments, words_per_example=200):\n",
    "    banned_stream = []\n",
    "    notbanned_stream = []\n",
    "    \n",
    "    for comm in comments:\n",
    "        if comm[\"banned\"]:\n",
    "            banned_stream += word_tokenize(comm[\"body\"])\n",
    "        else:\n",
    "            notbanned_stream += word_tokenize(comm[\"body\"])\n",
    "    \n",
    "    banned_examples = list(divide_chunks(banned_stream, words_per_example))\n",
    "    notbanned_examples = list(divide_chunks(notbanned_stream, words_per_example))\n",
    "\n",
    "    return banned_examples, notbanned_examples\n",
    "\n",
    "def chunkify_subreddit(subreddit, words_per_example=200):\n",
    "    banned = subreddit[0][\"banned\"]\n",
    "    my_subreddit = subreddit[0][\"subreddit\"]\n",
    "    \n",
    "    stream = []\n",
    "    for comm in subreddit:\n",
    "        stream += word_tokenize(comm[\"body\"])\n",
    "    \n",
    "    chunks = list(divide_chunks(stream, words_per_example))\n",
    "    return [{\"subreddit\": my_subreddit, \"banned\": banned, \"words\": chunk} for chunk in chunks]\n",
    "\n",
    "def chunkify_subreddits(subreddits, words_per_example=200):\n",
    "    return [chunkify_subreddit(subreddit, words_per_example) for subreddit in subreddits]\n",
    "\n",
    "def group_test_subreddits(test, words_per_example=200):\n",
    "    subreddit_to_comments = dict()\n",
    "    for comm in test:\n",
    "        if comm[\"subreddit\"] in subreddit_to_comments:\n",
    "            subreddit_to_comments[comm[\"subreddit\"]].append(comm)\n",
    "        else:\n",
    "            subreddit_to_comments[comm[\"subreddit\"]] = [comm]\n",
    "\n",
    "    return subreddit_to_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_COMMENTS = \"Data/Cached/2016_10_10M_comments.pkl\"\n",
    "RATIO = 0.8\n",
    "BLOCK_SIZE = 50000\n",
    "WORDS_PER_EXAMPLE = 200\n",
    "\n",
    "try:\n",
    "    comments = pickle.load(open(CACHED_COMMENTS, \"rb\"))\n",
    "    print(\"Loaded from cache\")\n",
    "\n",
    "except:\n",
    "    print(\"Failed to load from cache\")\n",
    "    data_folder = \"Data/\"\n",
    "\n",
    "    banned_subreddits = read_banned_subreddits(data_folder + \"banned-subreddits.txt\")\n",
    "    comments  = get_subreddit_data(data_folder + \"RC_2016-10.bz2\", banned_subreddits, max_comments=int(1e7), print_freq=10000)   \n",
    "    with open(CACHED_COMMENTS, \"wb\") as f:\n",
    "        pickle.dump(comments, f)\n",
    "print(\"Shuffling\")\n",
    "random.shuffle(comments)\n",
    "split_idx = int(len(comments) * RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [07:15<00:00,  2.89s/it]\n"
     ]
    }
   ],
   "source": [
    "BLOCK_SIZE = 50000\n",
    "train = comments[:split_idx]\n",
    "train_blocks = list(divide_chunks(train, BLOCK_SIZE))\n",
    "train_res = Parallel(n_jobs=-1)(delayed(chunkify_train)(block, WORDS_PER_EXAMPLE) for block in tqdm(train_blocks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:46<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "FNAME = \"Data/Generated/200_words_10M_%s.csv\"\n",
    "fbanned = open(FNAME%\"banned\", \"w\")\n",
    "fnotbanned = open(FNAME%\"notbanned\", \"w\")\n",
    "\n",
    "writer_banned  = csv.writer(fbanned)\n",
    "writer_notbanned  = csv.writer(fnotbanned)\n",
    "\n",
    "\n",
    "writer_banned.writerow([\"words\"])\n",
    "writer_notbanned.writerow([\"words\"])\n",
    "\n",
    "\n",
    "for banned_words_set, not_banned_words_set in tqdm(train_res):\n",
    "    for banned_words in banned_words_set:\n",
    "        writer_banned.writerow([' '.join(banned_words)])\n",
    "    for not_banned_words in not_banned_words_set:\n",
    "        writer_notbanned.writerow([' '.join(not_banned_words)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = comments[split_idx:]\n",
    "test_subreddit_to_comments = group_test_subreddits(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 100.29it/s]\n"
     ]
    }
   ],
   "source": [
    "test_blocks = list(divide_chunks(list(test_subreddit_to_comments.values()), int(len(test_subreddit_to_comments) / 31) + 1))\n",
    "test_res = Parallel(n_jobs=-1)(delayed(chunkify_subreddits)(subreddits, WORDS_PER_EXAMPLE)\n",
    "                        for subreddits in tqdm(test_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21057"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_subreddit_to_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = \"Data/Generated/200_words_10M_test.csv\"\n",
    "ftest = open(FNAME, \"w\")\n",
    "\n",
    "writer_test = csv.writer(ftest)\n",
    "writer_test.writerow([\"subreddit\", \"banned\", \"words\"])\n",
    "for batch in test_res:\n",
    "    for subreddit in batch:\n",
    "        for example in subreddit:\n",
    "            writer_test.writerow([example[\"subreddit\"], example[\"banned\"], ' '.join(example[\"words\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

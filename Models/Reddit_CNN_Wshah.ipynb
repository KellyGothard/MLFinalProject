{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin importing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing\n",
      "loading w2v\n",
      "loaded from pickle\n",
      "Done loading w2v\n",
      "Loading training / testing data from pickles\n",
      "Loaded.\n",
      "Training data consists of 200 words per training example.\n",
      "   banned                                             tokens\n",
      "0       0  [top, tier, isn, t, that, small, there, are, a...\n",
      "1       1  [m, actually, checking, out, the, thread, now,...\n",
      "2       1  [still, requires, input, deleted, just, got, t...\n",
      "3       0  [min, in, his, jungle, c9, will, punish, the, ...\n",
      "4       1  [well, ahead, of, the, curve, in, the, 1980s, ...\n",
      "   banned                                             tokens\n",
      "0       0  [wan, you, will, never, find, a, more, wretche...\n",
      "1       0  [re, right, i, ll, probably, delete, the, orig...\n",
      "2       0  [lose, ng, konting, weight, but, then, nakita,...\n",
      "3       0  [s, a, bad, thing, and, definitely, not, an, o...\n",
      "4       1  [and, isn, t, feasible, for, anything, over, l...\n",
      "18000000 words total, with a vocabulary size of 263939\n",
      "Max sentence length is 200\n",
      "Found 263939 unique tokens.\n",
      "(263940, 300)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin importing\")\n",
    "# imports + set random seeds.\n",
    "SEED = 0\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "# rest of the imports.\n",
    "# native packages\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "# third party.\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Done importing\")\n",
    "\n",
    "# Load W2V\n",
    "W2V_Pickle = \"../Data/Cached/w2v.p\"\n",
    "print(\"loading w2v\")\n",
    "try:\n",
    "    w2v_model = pickle.load(open(W2V_Pickle, \"rb\"))\n",
    "    print(\"loaded from pickle\")\n",
    "except:\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../Data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    pickle.dump(w2v_model, open(W2V_Pickle, \"wb\"))\n",
    "    print(\"loaded from model file\")\n",
    "\n",
    "print(\"Done loading w2v\")\n",
    "\n",
    "print(\"Loading training / testing data from pickles\")\n",
    "\n",
    "TRAIN_DATA = \"../Data/Generated/RC_2016-10_Train.pkl\"\n",
    "TEST_DATA = \"../Data/Generated/RC_2016-10_Test.pkl\"\n",
    "\n",
    "postsTrain = pd.read_pickle(TRAIN_DATA)\n",
    "postsTest = pd.read_pickle(TEST_DATA)\n",
    "\n",
    "print(\"Loaded.\")\n",
    "\n",
    "SEQ_LEN = len(postsTrain[\"tokens\"].values[0])\n",
    "print(\"Training data consists of %d words per training example.\"%SEQ_LEN)\n",
    "\n",
    "print(postsTrain.head())\n",
    "print(postsTest.head())\n",
    "\n",
    "y_train = postsTrain[\"banned\"].values\n",
    "y_test = postsTest[\"banned\"].values\n",
    "X_train = postsTrain[\"tokens\"].values\n",
    "X_test = postsTest[\"tokens\"].values\n",
    "\n",
    "all_words = [word for tokens in X_train for word in tokens]\n",
    "all_sentence_lengths = [SEQ_LEN]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))\n",
    "\n",
    "\n",
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = len(ALL_VOCAB) # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = max(all_sentence_lengths) # max number of words in a comment to use\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "\n",
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(train_cnn_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training examples.\n",
      "type, num loaded, w2v misses\n",
      "train 0 0\n",
      "train 1000 4663\n",
      "train 2000 8383\n",
      "train 3000 11940\n",
      "train 4000 15460\n",
      "train 5000 18684\n",
      "train 6000 21993\n",
      "train 7000 25148\n",
      "train 8000 28451\n",
      "train 9000 31157\n",
      "train 10000 34097\n",
      "train 11000 37016\n",
      "train 12000 39967\n",
      "train 13000 42698\n",
      "train 14000 45136\n",
      "train 15000 47648\n",
      "train 16000 50164\n",
      "train 17000 52455\n",
      "train 18000 55158\n",
      "train 19000 57525\n",
      "train 20000 60040\n",
      "train 21000 62372\n",
      "train 22000 64929\n",
      "train 23000 67281\n",
      "train 24000 69446\n",
      "train 25000 71494\n",
      "train 26000 73647\n",
      "train 27000 75809\n",
      "train 28000 78101\n",
      "train 29000 80191\n",
      "train 30000 82170\n",
      "train 31000 84115\n",
      "train 32000 86478\n",
      "train 33000 88536\n",
      "train 34000 90849\n",
      "train 35000 92831\n",
      "train 36000 94846\n",
      "train 37000 96707\n",
      "train 38000 98857\n",
      "train 39000 100978\n",
      "train 40000 102832\n",
      "train 41000 105023\n",
      "train 42000 106844\n",
      "train 43000 108801\n",
      "train 44000 110839\n",
      "train 45000 112858\n",
      "train 46000 114824\n",
      "train 47000 116707\n",
      "train 48000 118777\n",
      "train 49000 120761\n",
      "train 50000 122842\n",
      "train 51000 125203\n",
      "train 52000 126943\n",
      "train 53000 128726\n",
      "train 54000 130359\n",
      "train 55000 132230\n",
      "train 56000 133938\n",
      "train 57000 135765\n",
      "train 58000 137525\n",
      "train 59000 139215\n",
      "train 60000 140859\n",
      "train 61000 142565\n",
      "train 62000 144636\n",
      "train 63000 146455\n",
      "train 64000 148081\n",
      "train 65000 149765\n",
      "train 66000 151481\n",
      "train 67000 153074\n",
      "train 68000 154863\n",
      "train 69000 156402\n",
      "train 70000 158126\n",
      "train 71000 159873\n",
      "train 72000 161432\n",
      "train 73000 162886\n",
      "train 74000 164452\n",
      "train 75000 165937\n",
      "train 76000 167509\n",
      "train 77000 169193\n",
      "train 78000 170664\n",
      "train 79000 172245\n",
      "train 80000 173663\n",
      "train 81000 175353\n",
      "train 82000 176948\n",
      "train 83000 178395\n",
      "train 84000 179914\n",
      "train 85000 181740\n",
      "train 86000 183309\n",
      "train 87000 184789\n",
      "train 88000 186301\n",
      "train 89000 187852\n",
      "test 0 189575\n",
      "test 1000 190970\n",
      "test 2000 192554\n",
      "test 3000 194186\n",
      "test 4000 195595\n",
      "test 5000 197150\n",
      "test 6000 198455\n",
      "test 7000 199933\n",
      "test 8000 201302\n",
      "test 9000 202760\n"
     ]
    }
   ],
   "source": [
    "w2v_model_tmp = {}\n",
    "\n",
    "def get_word_vec(word):\n",
    "\n",
    "    if word in w2v_model:\n",
    "        return w2v_model[word]\n",
    "    elif word in w2v_model_tmp:\n",
    "        return w2v_model_tmp[word]\n",
    "    else:\n",
    "        w2v_model_tmp[word] = np.random.rand(EMBEDDING_DIM)\n",
    "        return w2v_model_tmp[word]\n",
    "        \n",
    "X_train_cnn = np.zeros((len(X_train), SEQ_LEN, EMBEDDING_DIM))\n",
    "\n",
    "X_test_cnn = np.zeros((len(X_test), SEQ_LEN, EMBEDDING_DIM))\n",
    "\n",
    "print(\"Generating training examples.\")\n",
    "print(\"type, num loaded, w2v misses\")\n",
    "for i in range(X_train_cnn.shape[0]):\n",
    "    if i %1000==0: print(\"train\", i, len(w2v_model_tmp))\n",
    "    for j in range(SEQ_LEN):\n",
    "        word_vec = get_word_vec( X_train[i][j])\n",
    "        X_train_cnn[i, j, :] = word_vec\n",
    "\n",
    "\n",
    "for i in range(X_test_cnn.shape[0]):\n",
    "    if i %1000==0: print(\"test\", i, len(w2v_model_tmp))\n",
    "    for j in range(SEQ_LEN):\n",
    "        word_vec = get_word_vec( X_test[i][j])\n",
    "        X_test_cnn[i, j, :] = word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 196, 128)          192128    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 98, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 98, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 92, 128)           114816    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 46, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 32, 128)           245888    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 2, 128)            245888    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 832,001\n",
      "Trainable params: 832,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "90000/90000 [==============================] - 85s 940us/step - loss: 0.5938 - acc: 0.6506 - val_loss: 0.4544 - val_acc: 0.7856\n",
      "Epoch 2/4\n",
      "90000/90000 [==============================] - 68s 753us/step - loss: 0.3719 - acc: 0.8349 - val_loss: 0.3025 - val_acc: 0.8702\n",
      "Epoch 3/4\n",
      "90000/90000 [==============================] - 86s 951us/step - loss: 0.3070 - acc: 0.8691 - val_loss: 0.4195 - val_acc: 0.7869\n",
      "Epoch 4/4\n",
      "90000/90000 [==============================] - 102s 1ms/step - loss: 0.2690 - acc: 0.8877 - val_loss: 0.2468 - val_acc: 0.8952\n",
      "Training Accuracy: 0.9246\n",
      "Testing Accuracy:  0.8952\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation=\"relu\", input_shape=(SEQ_LEN, EMBEDDING_DIM)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=7, activation=\"relu\", input_shape=(SEQ_LEN, EMBEDDING_DIM)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=15, activation=\"relu\", input_shape=(SEQ_LEN, EMBEDDING_DIM)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=15, activation=\"relu\", input_shape=(SEQ_LEN, EMBEDDING_DIM)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_cnn, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_data=(X_test_cnn, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_train_cnn, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_cnn, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATA\n",
      "Normalized values:\n",
      "      0     1\n",
      "0  0.89  0.11\n",
      "1  0.10  0.90\n",
      "\n",
      "Raw values:\n",
      "      0     1\n",
      "0  4433   554\n",
      "1   494  4519\n",
      "row: what should have been predicted\n",
      "column: what was predicted\n",
      "\n",
      "TRAIN DATA\n",
      "Normalized values:\n",
      "      0     1\n",
      "0  0.92  0.08\n",
      "1  0.07  0.93\n",
      "\n",
      "Raw values:\n",
      "       0      1\n",
      "0  41277   3736\n",
      "1   3046  41941\n",
      "row: what should have been predicted\n",
      "column: what was predicted\n"
     ]
    }
   ],
   "source": [
    "classes = [0,1]\n",
    "\n",
    "print(\"TEST DATA\")\n",
    "y_pred = np.array([1 if prd > 0.5 else 0 for prd in model.predict(X_test_cnn)])\n",
    "sess = tf.compat.v1.Session()\n",
    "con_mat = sess.run(tf.math.confusion_matrix(labels=y_test, predictions=y_pred))\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "\n",
    "con_mat_df_norm = pd.DataFrame(con_mat_norm,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "\n",
    "print(\"Normalized values:\")\n",
    "print(con_mat_df_norm)\n",
    "print(\"\\nRaw values:\")\n",
    "print(con_mat_df)\n",
    "\n",
    "print(\"row: what should have been predicted\")\n",
    "print(\"column: what was predicted\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"TRAIN DATA\")\n",
    "y_pred = np.array([1 if prd > 0.5 else 0 for prd in model.predict(X_train_cnn)])\n",
    "\n",
    "con_mat = sess.run(tf.math.confusion_matrix(labels=y_train, predictions=y_pred))\n",
    "\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df_norm = pd.DataFrame(con_mat_norm,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "print(\"Normalized values:\")\n",
    "print(con_mat_df_norm)\n",
    "print(\"\\nRaw values:\")\n",
    "print(con_mat_df)\n",
    "print(\"row: what should have been predicted\")\n",
    "print(\"column: what was predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

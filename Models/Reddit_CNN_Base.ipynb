{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin importing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing\n",
      "loading w2v\n",
      "loaded from pickle\n",
      "Done loading w2v\n",
      "Loading training / testing data from pickles\n",
      "Loaded.\n",
      "Training data consists of 200 words per training example.\n",
      "   banned                                             tokens\n",
      "0       1  [was, performed, automatically, please, contac...\n",
      "1       0  [levi, and, amira, hass, have, written, so, mu...\n",
      "2       0  [he, isnt, dead, maybe, hes, just, in, another...\n",
      "3       1  [benghazi, or, the, clinton, foundation, takin...\n",
      "4       0  [hilton, mike, evans, jordan, matthews, davont...\n",
      "   banned                                             tokens\n",
      "0       1  [tonight, we, wuz, peruvian, 10, s, a, bigger,...\n",
      "1       1  [but, this, to, my, knowledge, is, why, well, ...\n",
      "2       0  [than, the, wrx, they, come, out, to, the, eve...\n",
      "3       1  [but, i, m, talking, only, 1, 2, per, test, th...\n",
      "4       0  [immensely, beautiful, you, definitely, have, ...\n",
      "18000000 words total, with a vocabulary size of 263457\n",
      "Max sentence length is 200\n",
      "Found 263457 unique tokens.\n",
      "(263458, 300)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# inspired by:\n",
    "# https://towardsdatascience.com/natural-language-processing-classification-using-deep-learning-and-word2vec-50cbadd3bd6a\n",
    "\n",
    "print(\"Begin importing\")\n",
    "# imports + set random seeds.\n",
    "SEED = 0\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "# rest of the imports.\n",
    "# native packages\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "# third party.\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Done importing\")\n",
    "\n",
    "# Load W2V\n",
    "W2V_Pickle = \"../Data/Cached/w2v.p\"\n",
    "print(\"loading w2v\")\n",
    "try:\n",
    "    w2v_model = pickle.load(open(W2V_Pickle, \"rb\"))\n",
    "    print(\"loaded from pickle\")\n",
    "except:\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../Data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    pickle.dump(w2v_model, open(W2V_Pickle, \"wb\"))\n",
    "    print(\"loaded from model file\")\n",
    "\n",
    "print(\"Done loading w2v\")\n",
    "\n",
    "print(\"Loading training / testing data from pickles\")\n",
    "\n",
    "TRAIN_DATA = \"../Data/Generated/RC_2016-10_Train.pkl\"\n",
    "TEST_DATA = \"../Data/Generated/RC_2016-10_Test.pkl\"\n",
    "\n",
    "postsTrain = pd.read_pickle(TRAIN_DATA)\n",
    "postsTest = pd.read_pickle(TEST_DATA)\n",
    "\n",
    "print(\"Loaded.\")\n",
    "\n",
    "SEQ_LEN = len(postsTrain[\"tokens\"].values[0])\n",
    "print(\"Training data consists of %d words per training example.\"%SEQ_LEN)\n",
    "\n",
    "print(postsTrain.head())\n",
    "print(postsTest.head())\n",
    "\n",
    "y_train = postsTrain[\"banned\"].values\n",
    "y_test = postsTest[\"banned\"].values\n",
    "X_train = postsTrain[\"tokens\"].values\n",
    "X_test = postsTest[\"tokens\"].values\n",
    "\n",
    "all_words = [word for tokens in X_train for word in tokens]\n",
    "all_sentence_lengths = [SEQ_LEN]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))\n",
    "\n",
    "\n",
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = len(ALL_VOCAB) # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = max(all_sentence_lengths) # max number of words in a comment to use\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "\n",
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print(train_cnn_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=False, extra_conv=True):\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Based on Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3, 3, 3, 4, 5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate([convs[0],convs[1],convs[2], convs[3], convs[4]],axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)\n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0)\n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adadelta',\n",
    "                      metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 200, 300)     79037400    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 198, 128)     115328      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 198, 128)     115328      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 198, 128)     115328      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 197, 128)     153728      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 196, 128)     192128      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 66, 128)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 66, 128)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 66, 128)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 65, 128)      0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 65, 128)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 328, 128)     0           max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "                                                                 max_pooling1d_15[0][0]           \n",
      "                                                                 max_pooling1d_16[0][0]           \n",
      "                                                                 max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 328, 128)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 41984)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          5374080     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            129         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 85,103,449\n",
      "Trainable params: 6,066,049\n",
      "Non-trainable params: 79,037,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "90000/90000 [==============================] - 18s 198us/step - loss: 7.9544 - acc: 0.5007 - val_loss: 8.0844 - val_acc: 0.4929\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================] - 16s 182us/step - loss: 0.7544 - acc: 0.7607 - val_loss: 0.4359 - val_acc: 0.7988\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================] - 17s 186us/step - loss: 0.3153 - acc: 0.8675 - val_loss: 0.3764 - val_acc: 0.8278\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================] - 17s 192us/step - loss: 0.2565 - acc: 0.8950 - val_loss: 0.2809 - val_acc: 0.8843\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================] - 17s 194us/step - loss: 0.2060 - acc: 0.9185 - val_loss: 0.2666 - val_acc: 0.8913\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================] - 17s 192us/step - loss: 0.1569 - acc: 0.9397 - val_loss: 0.5284 - val_acc: 0.8068\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================] - 17s 192us/step - loss: 0.1181 - acc: 0.9541 - val_loss: 0.3228 - val_acc: 0.8851\n",
      "Epoch 8/10\n",
      "90000/90000 [==============================] - 17s 193us/step - loss: 0.0923 - acc: 0.9651 - val_loss: 0.3141 - val_acc: 0.8928\n",
      "Epoch 9/10\n",
      "90000/90000 [==============================] - 17s 188us/step - loss: 0.0731 - acc: 0.9732 - val_loss: 0.3296 - val_acc: 0.8943\n",
      "Epoch 10/10\n",
      "90000/90000 [==============================] - 17s 191us/step - loss: 0.0614 - acc: 0.9767 - val_loss: 0.3523 - val_acc: 0.8920\n",
      "Training Accuracy: 0.9984\n",
      "Testing Accuracy:  0.8920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, False)\n",
    "history = model.fit(train_cnn_data, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                       validation_data=(test_cnn_data, y_test) )\n",
    "\n",
    "loss, accuracy = model.evaluate(train_cnn_data, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_cnn_data, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1]\n",
    "\n",
    "print(\"TEST DATA\")\n",
    "y_pred = np.array([1 if prd > 0.5 else 0 for prd in model.predict(test_cnn_data)])\n",
    "sess = tf.compat.v1.Session()\n",
    "con_mat = sess.run(tf.math.confusion_matrix(labels=y_test, predictions=y_pred))\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "\n",
    "con_mat_df_norm = pd.DataFrame(con_mat_norm,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "\n",
    "print(\"Normalized values:\")\n",
    "print(con_mat_df_norm)\n",
    "print(\"\\nRaw values:\")\n",
    "print(con_mat_df)\n",
    "\n",
    "print(\"row: what should have been predicted\")\n",
    "print(\"column: what was predicted\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"TRAIN DATA\")\n",
    "y_pred = np.array([1 if prd > 0.5 else 0 for prd in model.predict(train_cnn_data)])\n",
    "\n",
    "con_mat = sess.run(tf.math.confusion_matrix(labels=y_train, predictions=y_pred))\n",
    "\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df_norm = pd.DataFrame(con_mat_norm,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "print(\"Normalized values:\")\n",
    "print(con_mat_df_norm)\n",
    "print(\"\\nRaw values:\")\n",
    "print(con_mat_df)\n",
    "print(\"row: what should have been predicted\")\n",
    "print(\"column: what was predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

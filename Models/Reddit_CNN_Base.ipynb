{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin importing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done importing\n",
      "loading w2v\n",
      "loaded from pickle\n",
      "Done loading w2v\n",
      "Loading training / testing data from pickles\n",
      "Loaded.\n",
      "Training data consists of 200 words per training example.\n",
      "   banned                                             tokens\n",
      "0       0  [top, tier, isn, t, that, small, there, are, a...\n",
      "1       1  [m, actually, checking, out, the, thread, now,...\n",
      "2       1  [still, requires, input, deleted, just, got, t...\n",
      "3       0  [min, in, his, jungle, c9, will, punish, the, ...\n",
      "4       1  [well, ahead, of, the, curve, in, the, 1980s, ...\n",
      "   banned                                             tokens\n",
      "0       0  [wan, you, will, never, find, a, more, wretche...\n",
      "1       0  [re, right, i, ll, probably, delete, the, orig...\n",
      "2       0  [lose, ng, konting, weight, but, then, nakita,...\n",
      "3       0  [s, a, bad, thing, and, definitely, not, an, o...\n",
      "4       1  [and, isn, t, feasible, for, anything, over, l...\n",
      "18000000 words total, with a vocabulary size of 263939\n",
      "Max sentence length is 200\n",
      "Found 263939 unique tokens.\n",
      "(263940, 300)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# inspired by:\n",
    "# https://towardsdatascience.com/natural-language-processing-classification-using-deep-learning-and-word2vec-50cbadd3bd6a\n",
    "\n",
    "print(\"Begin importing\")\n",
    "# imports + set random seeds.\n",
    "SEED = 0\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "# rest of the imports.\n",
    "# native packages\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from time import time\n",
    "\n",
    "# third party.\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Done importing\")\n",
    "\n",
    "# Load W2V\n",
    "W2V_Pickle = \"../Data/Cached/w2v.p\"\n",
    "print(\"loading w2v\")\n",
    "try:\n",
    "    w2v_model = pickle.load(open(W2V_Pickle, \"rb\"))\n",
    "    print(\"loaded from pickle\")\n",
    "except:\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../Data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    pickle.dump(w2v_model, open(W2V_Pickle, \"wb\"))\n",
    "    print(\"loaded from model file\")\n",
    "\n",
    "print(\"Done loading w2v\")\n",
    "\n",
    "print(\"Loading training / testing data from pickles\")\n",
    "\n",
    "TRAIN_DATA = \"../Data/Generated/RC_2016-10_Train.pkl\"\n",
    "TEST_DATA = \"../Data/Generated/RC_2016-10_Test.pkl\"\n",
    "\n",
    "postsTrain = pd.read_pickle(TRAIN_DATA)\n",
    "postsTest = pd.read_pickle(TEST_DATA)\n",
    "\n",
    "print(\"Loaded.\")\n",
    "\n",
    "SEQ_LEN = len(postsTrain[\"tokens\"].values[0])\n",
    "print(\"Training data consists of %d words per training example.\"%SEQ_LEN)\n",
    "\n",
    "print(postsTrain.head())\n",
    "print(postsTest.head())\n",
    "\n",
    "y_train = postsTrain[\"banned\"].values\n",
    "y_test = postsTest[\"banned\"].values\n",
    "X_train = postsTrain[\"tokens\"].values\n",
    "X_test = postsTest[\"tokens\"].values\n",
    "\n",
    "all_words = [word for tokens in X_train for word in tokens]\n",
    "all_sentence_lengths = [SEQ_LEN]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))\n",
    "\n",
    "\n",
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = len(ALL_VOCAB) # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = max(all_sentence_lengths) # max number of words in a comment to use\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "\n",
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print(train_cnn_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     79182000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 128)     115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 198, 128)     115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 198, 128)     115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 196, 128)     192128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 66, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 66, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 66, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 65, 128)      0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 65, 128)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 328, 128)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 328, 128)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 41984)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          5374080     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 85,248,049\n",
      "Trainable params: 6,066,049\n",
      "Non-trainable params: 79,182,000\n",
      "__________________________________________________________________________________________________\n",
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "90000/90000 [==============================] - 23s 252us/step - loss: 0.5944 - acc: 0.7332 - val_loss: 0.3792 - val_acc: 0.8286\n",
      "Epoch 2/3\n",
      "90000/90000 [==============================] - 18s 204us/step - loss: 0.3234 - acc: 0.8624 - val_loss: 0.2835 - val_acc: 0.8828\n",
      "Epoch 3/3\n",
      "90000/90000 [==============================] - 18s 203us/step - loss: 0.2594 - acc: 0.8938 - val_loss: 0.2975 - val_acc: 0.8744\n",
      "Training Accuracy: 0.9140\n",
      "Testing Accuracy:  0.8744\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=False, extra_conv=True):\n",
    "\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights=[embeddings],\n",
    "                                input_length=max_sequence_length,\n",
    "                                trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Based on Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3, 3, 3, 4, 5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate([convs[0],convs[1],convs[2], convs[3], convs[4]],axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)\n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0)\n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adadelta',\n",
    "                      metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, False)\n",
    "history = model.fit(train_cnn_data, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                       validation_data=(test_cnn_data, y_test) )\n",
    "\n",
    "loss, accuracy = model.evaluate(train_cnn_data, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_cnn_data, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATA\n",
      "Normalized values:\n",
      "      0     1\n",
      "0  0.96  0.04\n",
      "1  0.21  0.79\n",
      "\n",
      "Raw values:\n",
      "      0     1\n",
      "0  4775   212\n",
      "1  1044  3969\n",
      "row: what should have been predicted\n",
      "column: what was predicted\n",
      "\n",
      "TRAIN DATA\n",
      "Normalized values:\n",
      "      0     1\n",
      "0  0.98  0.02\n",
      "1  0.15  0.85\n",
      "\n",
      "Raw values:\n",
      "       0      1\n",
      "0  44071    942\n",
      "1   6795  38192\n",
      "row: what should have been predicted\n",
      "column: what was predicted\n"
     ]
    }
   ],
   "source": [
    "classes = [0,1]\n",
    "\n",
    "print(\"TEST DATA\")\n",
    "y_pred = np.array([1 if prd > 0.5 else 0 for prd in model.predict(test_cnn_data)])\n",
    "sess = tf.compat.v1.Session()\n",
    "con_mat = sess.run(tf.math.confusion_matrix(labels=y_test, predictions=y_pred))\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "\n",
    "con_mat_df_norm = pd.DataFrame(con_mat_norm,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "\n",
    "print(\"Normalized values:\")\n",
    "print(con_mat_df_norm)\n",
    "print(\"\\nRaw values:\")\n",
    "print(con_mat_df)\n",
    "\n",
    "print(\"row: what should have been predicted\")\n",
    "print(\"column: what was predicted\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"TRAIN DATA\")\n",
    "y_pred = np.array([1 if prd > 0.5 else 0 for prd in model.predict(train_cnn_data)])\n",
    "\n",
    "con_mat = sess.run(tf.math.confusion_matrix(labels=y_train, predictions=y_pred))\n",
    "\n",
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df_norm = pd.DataFrame(con_mat_norm,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                              index=classes,\n",
    "                              columns=classes)\n",
    "print(\"Normalized values:\")\n",
    "print(con_mat_df_norm)\n",
    "print(\"\\nRaw values:\")\n",
    "print(con_mat_df)\n",
    "print(\"row: what should have been predicted\")\n",
    "print(\"column: what was predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### Always import all needed libraries in the first cell\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Reshape\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "\n",
    "np.random.seed(1) # this sets the seed so that the runs are consistent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test percent lost: 5.63\n"
     ]
    }
   ],
   "source": [
    "dftrain_banned = pd.read_csv(\"../Data/Generated/200_words_10M_banned.csv\", delimiter=',')\n",
    "dftrain_banned.insert(0, \"banned\", 1)\n",
    "\n",
    "dftrain_notbanned = pd.read_csv(\"../Data/Generated/200_words_10M_notbanned.csv\", delimiter=',')\n",
    "dftrain_notbanned.insert(0, \"banned\", 0)\n",
    "\n",
    "dfTest = pd.read_csv(\"../Data/Generated/200_words_10M_test.csv\", delimiter=',')\n",
    "dfTest = dfTest.sample(frac=1)\n",
    "\n",
    "dfTest[\"split\"] = dfTest[\"words\"].map(lambda x: x.split(\" \"), na_action='ignore')\n",
    "dfTest[\"word_cnt\"] = dfTest[\"split\"].map(lambda x: len(x), na_action='ignore')\n",
    "print(\"Test percent lost: %.2f\" % (100*len(dfTest[dfTest[\"word_cnt\"] != 200])/ len(dfTest)))\n",
    "dfTest = dfTest[dfTest[\"word_cnt\"] == 200]\n",
    "\n",
    "dfTest_banned = dfTest[dfTest[\"banned\"]]\n",
    "dfTest_notbanned = dfTest[dfTest[\"banned\"] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BALANCE_RATIO = 25\n",
    "TRAIN_N_COMMENTS = int(len(dftrain_banned)/2)\n",
    "TEST_N_COMMENTS = int(len(dfTest_banned)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest_balanced = pd.concat([dfTest_banned.head(n=TEST_N_COMMENTS), dfTest_notbanned.head(n=BALANCE_RATIO*TEST_N_COMMENTS)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = pd.concat([dftrain_banned.head(n=TRAIN_N_COMMENTS), dftrain_notbanned.head(n=BALANCE_RATIO*TRAIN_N_COMMENTS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train percent lost: 0.09\n"
     ]
    }
   ],
   "source": [
    "dfTrain[\"split\"] = dfTrain[\"words\"].apply(lambda x: x.split(\" \"))\n",
    "dfTrain[\"word_cnt\"] = dfTrain[\"split\"].apply(lambda x: len(x))\n",
    "print(\"Train percent lost: %.2f\" % (100*len(dfTrain[dfTrain[\"word_cnt\"] != 200])/ len(dfTrain)))\n",
    "dfTrain = dfTrain[dfTrain[\"word_cnt\"]== 200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banned</th>\n",
       "      <th>words</th>\n",
       "      <th>split</th>\n",
       "      <th>word_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12420</th>\n",
       "      <td>0</td>\n",
       "      <td>both ) . Did Evan switch with Daniel ? ! I lov...</td>\n",
       "      <td>[both, ), ., Did, Evan, switch, with, Daniel, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86335</th>\n",
       "      <td>0</td>\n",
       "      <td>a number of equally probably hypothetical situ...</td>\n",
       "      <td>[a, number, of, equally, probably, hypothetica...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19183</th>\n",
       "      <td>0</td>\n",
       "      <td>that negatively , everyone is ignorant to diff...</td>\n",
       "      <td>[that, negatively, ,, everyone, is, ignorant, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53631</th>\n",
       "      <td>0</td>\n",
       "      <td>other users posting on this thread ( and **not...</td>\n",
       "      <td>[other, users, posting, on, this, thread, (, a...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25681</th>\n",
       "      <td>0</td>\n",
       "      <td>: //www.reddit.com/r/history/comments/55yktm/j...</td>\n",
       "      <td>[:, //www.reddit.com/r/history/comments/55yktm...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41986</th>\n",
       "      <td>0</td>\n",
       "      <td>of Sorcery ) 1 - Ganondorf ( Trident ) Bottom ...</td>\n",
       "      <td>[of, Sorcery, ), 1, -, Ganondorf, (, Trident, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74056</th>\n",
       "      <td>0</td>\n",
       "      <td>http : //i.imgur.com/M3m0WM4.png ) 2 . [ Why d...</td>\n",
       "      <td>[http, :, //i.imgur.com/M3m0WM4.png, ), 2, ., ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47694</th>\n",
       "      <td>0</td>\n",
       "      <td>I 'm all that nice or all that principled , an...</td>\n",
       "      <td>[I, 'm, all, that, nice, or, all, that, princi...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46256</th>\n",
       "      <td>0</td>\n",
       "      <td>enter the subway is what , $ 2 ? So do you thi...</td>\n",
       "      <td>[enter, the, subway, is, what, ,, $, 2, ?, So,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49440</th>\n",
       "      <td>0</td>\n",
       "      <td>was crying pretty hard when I thought about wh...</td>\n",
       "      <td>[was, crying, pretty, hard, when, I, thought, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       banned                                              words  \\\n",
       "12420       0  both ) . Did Evan switch with Daniel ? ! I lov...   \n",
       "86335       0  a number of equally probably hypothetical situ...   \n",
       "19183       0  that negatively , everyone is ignorant to diff...   \n",
       "53631       0  other users posting on this thread ( and **not...   \n",
       "25681       0  : //www.reddit.com/r/history/comments/55yktm/j...   \n",
       "41986       0  of Sorcery ) 1 - Ganondorf ( Trident ) Bottom ...   \n",
       "74056       0  http : //i.imgur.com/M3m0WM4.png ) 2 . [ Why d...   \n",
       "47694       0  I 'm all that nice or all that principled , an...   \n",
       "46256       0  enter the subway is what , $ 2 ? So do you thi...   \n",
       "49440       0  was crying pretty hard when I thought about wh...   \n",
       "\n",
       "                                                   split  word_cnt  \n",
       "12420  [both, ), ., Did, Evan, switch, with, Daniel, ...       200  \n",
       "86335  [a, number, of, equally, probably, hypothetica...       200  \n",
       "19183  [that, negatively, ,, everyone, is, ignorant, ...       200  \n",
       "53631  [other, users, posting, on, this, thread, (, a...       200  \n",
       "25681  [:, //www.reddit.com/r/history/comments/55yktm...       200  \n",
       "41986  [of, Sorcery, ), 1, -, Ganondorf, (, Trident, ...       200  \n",
       "74056  [http, :, //i.imgur.com/M3m0WM4.png, ), 2, ., ...       200  \n",
       "47694  [I, 'm, all, that, nice, or, all, that, princi...       200  \n",
       "46256  [enter, the, subway, is, what, ,, $, 2, ?, So,...       200  \n",
       "49440  [was, crying, pretty, hard, when, I, thought, ...       200  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = dfTrain.sample(frac=1)\n",
    "dfTrain.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(dfTrain[\"words\"])\n",
    "y_train = dfTrain[\"banned\"]\n",
    "\n",
    "X_test = vectorizer.transform(dfTest_balanced[\"words\"])\n",
    "y_test = dfTest_balanced[\"banned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97413, 287467)\n",
      "287467\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "print(X_train.shape)\n",
    "print(input_dim)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "for i in range(10):\n",
    "    model.add(layers.Dense(30, input_dim=input_dim, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dense_9_input (InputLayer)      (None, 287467)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 287467)       0           dense_9_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 287467)       0           dense_9_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 287467)       0           dense_9_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 287467)       0           dense_9_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 1)            8632731     lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Concatenate)          (None, 1)            0           sequential_3[1][0]               \n",
      "                                                                 sequential_3[2][0]               \n",
      "                                                                 sequential_3[3][0]               \n",
      "                                                                 sequential_3[4][0]               \n",
      "==================================================================================================\n",
      "Total params: 8,632,731\n",
      "Trainable params: 8,632,731\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=4, cpu_merge=False)\n",
    "parallel_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "parallel_model.summary()\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['acc',f1_m,precision_m, recall_m])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 97413 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "97413/97413 [==============================] - 199s 2ms/step - loss: 0.3157 - acc: 0.9573 - f1_m: 0.0046 - precision_m: 0.0081 - recall_m: 0.0054 - val_loss: 0.3141 - val_acc: 0.9600 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 2/10\n",
      "33792/97413 [=========>....................] - ETA: 1:54 - loss: 0.1958 - acc: 0.9616 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "\n",
    "history = parallel_model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test[:10000], y_test[:10000]),\n",
    "                    batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "def cross_val_keras(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_keras(model, X_test, y_test, 'Model')\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can probs ignore from here down\n",
    "\n",
    "banned_dict = {}\n",
    "\n",
    "for word in banned_words_no_punct.split(\" \"):\n",
    "    if word in banned_dict:\n",
    "        banned_dict[word] += 1\n",
    "    else:\n",
    "        banned_dict[word] = 1\n",
    "        \n",
    "print(dict(list(banned_dict.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Seriously': 52, 'The': 2331, 'pats': 6, 'havent': 245, 'won': 77}\n"
     ]
    }
   ],
   "source": [
    "not_banned_dict = {}\n",
    "\n",
    "for word in not_banned_words_no_punct.split(\" \"):\n",
    "    if word in not_banned_dict:\n",
    "        not_banned_dict[word] += 1\n",
    "    else:\n",
    "        not_banned_dict[word] = 1\n",
    "        \n",
    "print(dict(list(not_banned_dict.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'know': 39, 'that': 0, 'strenuous': 4, 'ever': 1}\n"
     ]
    }
   ],
   "source": [
    "banned_counts = {}\n",
    "for word in list(banned_dict.keys()):\n",
    "    if word in not_banned_dict:\n",
    "        banned_counts[word] = banned_dict[word] - not_banned_dict[word]\n",
    "        if banned_counts[word] < 0:\n",
    "            banned_counts[word] = 0\n",
    "    else:\n",
    "        banned_counts[word] = banned_dict[word]\n",
    "\n",
    "print(dict(list(banned_counts.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Seriously': 0, 'The': 733, 'pats': 6, 'havent': 37, 'won': 66}\n"
     ]
    }
   ],
   "source": [
    "not_banned_counts = {}\n",
    "for word in list(not_banned_dict.keys()):\n",
    "    if word in banned_dict:\n",
    "        not_banned_counts[word] = not_banned_dict[word] - banned_dict[word]\n",
    "        if not_banned_counts[word] < 0:\n",
    "            not_banned_counts[word] = 0\n",
    "    else:\n",
    "        not_banned_counts[word] = not_banned_dict[word]\n",
    "        \n",
    "print(dict(list(not_banned_counts.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the banned dictionary - how many times it shows up in the not banned dictionary\n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in banned_counts:\n",
    "            weight += banned_counts[word]\n",
    "        if word in not_banned_counts:\n",
    "            weight -= not_banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBannedCount(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the banned dictionary \n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in banned_counts:\n",
    "            weight += banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNotBannedCount(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the not banned dictionary \n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in not_banned_counts:\n",
    "            weight += not_banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "feature = []\n",
    "#labels = np.array(int)\n",
    "labels = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "feature3 = []\n",
    "count = 0\n",
    "for comment in banned_comments:\n",
    "    if count < 10000:\n",
    "        feature1.append(float(getBannedCount(comment)))\n",
    "        feature2.append(float(getNotBannedCount(comment)))\n",
    "        feature3.append(float(getCounts(comment)))\n",
    "        labels.append(float(1))\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "        #labels = np.append(labels, 1)\n",
    "        \n",
    "count = 0\n",
    "for comment in not_banned_comments:\n",
    "    if count < 10000:\n",
    "        feature1.append(float(getBannedCount(comment)))\n",
    "        feature2.append(float(getNotBannedCount(comment)))\n",
    "        feature3.append(float(getCounts(comment)))\n",
    "        labels.append(float(0))\n",
    "        #labels = np.append(labels, 0)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "feature.append(feature1)\n",
    "feature.append(feature2)\n",
    "feature.append(feature3)\n",
    "labels1 = []\n",
    "labels1.append(labels)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([np.array(xi) for xi in feature])\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.array([np.array(xi) for xi in labels1])\n",
    "type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (3, 20000)\n",
      "The shape of Y is: (1, 20000)\n",
      "I have 800 training sample!\n"
     ]
    }
   ],
   "source": [
    "shape_X = features.shape\n",
    "shape_Y = label.shape\n",
    "m = 2 * 400\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have %d training sample!' % (m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerSizes(X, Y):\n",
    "    \"\"\"\n",
    "    X -- input dataset of shape \n",
    "    Y -- labels of shape\n",
    "    \"\"\"\n",
    "    input_layer_size= X.shape[0]\n",
    "    hidden_layer_size= 4\n",
    "    output_layer_size= Y.shape[0]\n",
    "    # hardcode as 1 bc we have to \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    input_layer_size -- the size of the input layer\n",
    "    hidden_layer_size -- the size of the hidden layer\n",
    "    output_layer_size -- the size of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    return (input_layer_size, hidden_layer_size, output_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size-- size of the input layer\n",
    "    hidden_size -- size of the hidden layer\n",
    "    output_size-- size of the output layer\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)  # you can pick any seed in this case\n",
    "    \n",
    "    Weight1 = np.random.randn(hidden_size,input_size) * 0.01\n",
    "    Weight2 = np.random.randn(output_size,hidden_size) * 0.01\n",
    "    bias1 = np.zeros(shape=(hidden_size, 1))\n",
    "    bias2 = np.zeros(shape=(output_size, 1))\n",
    "    \n",
    "    parameters = {\"Weight1\": Weight1,\n",
    "                  \"bias1\": bias1,\n",
    "                  \"Weight2\": Weight2,\n",
    "                  \"bias2\": bias2}\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape \n",
    "                    b1 -- bias vector of shape \n",
    "                    W2 -- weight matrix of shape \n",
    "                    b2 -- bias vector of shape\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    X -- input data of size\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    bias1 = parameters['bias1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(Weight1,X) + bias1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(Weight2,A1) + bias2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    #Values needed in the backpropagation are stored in cache. Later, it will be given to back propagation.\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    Y -- \"true\" labels vector of shape \n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]  # number of example \n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y[0]) + np.multiply((1 - Y[0]), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    ### Remember that, if you want to use different cross-entropy loss, you need to change logprobs and cost accordingly\n",
    "    \n",
    "    cost = float(np.squeeze(cost))   \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters -- dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data \n",
    "    Y -- \"true\" labels vector \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Copy W1 and W2 from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    \n",
    "        \n",
    "    # Copy A1 and A2 from dictionary \"cache\".\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    #  calculate dW1, db1, dW2, db2. \n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(Weight2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradient = {\"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                \"dW2\": dW2,\n",
    "                \"db2\": db2}\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.5):\n",
    "    \"\"\"\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \"\"\"\n",
    "    # Copy the following parameter from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias1 = parameters['bias1']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # Copy each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    Weight1 = Weight1 - learning_rate * dW1\n",
    "    Weight2 = Weight2 - learning_rate * dW2\n",
    "    bias1 = bias1 - learning_rate * db1\n",
    "    bias2 = bias2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"Weight1\": Weight1,\n",
    "                  \"Weight2\": Weight2,\n",
    "                  \"bias1\": bias1,\n",
    "                  \"bias2\": bias2}\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "    #print(parameters)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, n_h, num_iterations = 1000, print_cost=True):\n",
    "    \"\"\"\n",
    "    X -- dataset\n",
    "    Y -- labels\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent\n",
    "    print_cost -- if True, print the cost in every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(4)\n",
    "    n_x = layerSizes(X, Y)[0]\n",
    "    n_y = layerSizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    Weight1 = parameters['Weight1']\n",
    "    bias1 = parameters['bias1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # gradient descent\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Call the Forward propagation with X, and parameters.\n",
    "        A2, cache = forward_prop(X, parameters)\n",
    "        \n",
    "        # Call the Cost function with A2, Y and parameters.\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Call Backpropagation with Inputs, parameters, cache, X and Y.\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Update gradient descent parameter with  parameters and grads and learning rate.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_prop(X,parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693369\n",
      "Cost after iteration 100: 0.691049\n",
      "Cost after iteration 200: 0.690562\n",
      "Cost after iteration 300: 0.690457\n",
      "Cost after iteration 400: 0.690391\n",
      "Cost after iteration 500: 0.690337\n",
      "Cost after iteration 600: 0.690291\n",
      "Cost after iteration 700: 0.690251\n",
      "Cost after iteration 800: 0.690216\n",
      "Cost after iteration 900: 0.690187\n",
      "Accuracy: 51%\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = model(features, label, n_h = 1, num_iterations = 1000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "# plot_decision_boundary(lambda x: predict(parameters, x.T), features, label[0])\n",
    "# plt.title(\"Decision Boundary for hidden layer size \" + str(4));\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, features)\n",
    "print ('Accuracy: %d' % float((np.dot(label,predictions.T) + np.dot(1-label,1-predictions.T))/float(label.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

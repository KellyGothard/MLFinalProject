{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Always import all needed libraries in the first cell\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVC\n",
    "#from planar_utils import plot_decision_boundary, sigmoid\n",
    "# from Dataset import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "np.random.seed(1) # this sets the seed so that the runs are consistent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notbanned = pd.read_csv(\"notbanned.csv\", delimiter=',')\n",
    "\n",
    "# pd.set_option('display.max_columns', 5)  # Set to actually print out the full columns, change if needed\n",
    "# # print(notbanned.head(n=10))\n",
    "\n",
    "# banned = pd.read_csv(\"banned.csv\", delimiter=',')\n",
    "\n",
    "# pd.set_option('display.max_columns', 5)  # Set to actually print out the full columns, change if needed\n",
    "# # print(banned.head(n=10))\n",
    "\n",
    "# banned_comments = []\n",
    "\n",
    "# for line in banned['body']:\n",
    "#     if \"I am a bot\" in str(line):\n",
    "#         #print(\"SKIPPING THIS LINE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "#         pass\n",
    "#     elif \"r/\" in str(line):\n",
    "#         pass\n",
    "#     elif '[deleted]' in str(line):\n",
    "#         pass\n",
    "#     else:\n",
    "#         for tempLine in str(line).split('\\n'):\n",
    "#             banned_comments.append(str(tempLine))\n",
    "\n",
    "# not_banned_comments = []\n",
    "\n",
    "# for line in notbanned['body']:\n",
    "#     if \"I am a bot\" in str(line):\n",
    "#         #print(\"SKIPPING THIS LINE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "#         pass\n",
    "#     elif \"r/\" in str(line):\n",
    "#         pass\n",
    "#     elif '[deleted]' in str(line):\n",
    "#         pass\n",
    "#     else:\n",
    "#         for tempLine in str(line).split('\\n'):\n",
    "#             not_banned_comments.append(str(tempLine))\n",
    "\n",
    "# exclude = set(punctuation)  # Keep a set of \"bad\" characters.\n",
    "# # then make a string of all the words in Obama and Romney tweets without punctuation\n",
    "# banned_words_no_punct = \" \".join([\"\".join(str(char) for char in text if char not in exclude) for text in banned_comments])\n",
    "# not_banned_words_no_punct = \" \".join([\"\".join(str(char) for char in text if char not in exclude) for text in not_banned_comments])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Xcomments = []\n",
    "# ycomments = []\n",
    "\n",
    "# count = 0\n",
    "# tempStr = ''\n",
    "# for line in banned_words_no_punct.split(' '):\n",
    "#     if count < 200:\n",
    "#         tempStr += (' ' + str(line))\n",
    "#         count += 1\n",
    "#     else:\n",
    "#         Xcomments.append(str(tempStr))\n",
    "#         ycomments.append(1)\n",
    "# #         print(tempStr)\n",
    "#         tempStr = str(line)\n",
    "# #         print(tempStr)\n",
    "#         count = 0\n",
    "#         tempStr =''\n",
    "\n",
    "# count = 0\n",
    "# tempStr = ''       \n",
    "# for line in not_banned_words_no_punct.split(' '):\n",
    "#     if count < 200:\n",
    "#         tempStr += (' ' + str(line))\n",
    "#         count += 1\n",
    "#     else:    \n",
    "#         Xcomments.append(str(tempStr))\n",
    "#         ycomments.append(0)\n",
    "# #         print(tempStr)\n",
    "#         tempStr = str(line)\n",
    "# #         print(tempStr)\n",
    "#         count = 0\n",
    "#         tempStr =''\n",
    "# vectorizer = TfidfVectorizer(min_df=0, lowercase=True)\n",
    "# X = vectorizer.fit_transform(Xcomments)\n",
    "# # print(vectorizer.get_feature_names())\n",
    "# # print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = pd.read_csv(\"../Data/RC_2016-10_200_words.csv\", delimiter=',')\n",
    "Xcomments = []\n",
    "ycomments = []\n",
    "index = 0\n",
    "for line in allData['txt']:\n",
    "    Xcomments.append(str(line))\n",
    "    ycomments.append(int(allData['banned'][index]))\n",
    "#     print(str(allData['banned'][index]))\n",
    "    index += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.transform(Xcomments).toarray()\n",
    "#not_banned_vect.transform(not_banned_comments).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "comments_train, comments_test, y_train, y_test = train_test_split(Xcomments, ycomments, test_size=0.25, random_state=400, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<75000x236321 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9822258 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(comments_train)\n",
    "\n",
    "X_train = vectorizer.transform(comments_train)\n",
    "X_test  = vectorizer.transform(comments_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "def cross_val(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# classifier = LogisticRegression()\n",
    "# classifier.fit(X_train, y_train)\n",
    "# score = classifier.score(X_test, y_test)\n",
    "# cross_val(classifier, X_test, y_test, \"Logistic Regression\" )\n",
    "\n",
    "# print(\"Accuracy:\", round(score, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75000, 236321)\n",
      "236321\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Reshape\n",
    "\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "print(X_train.shape)\n",
    "print(input_dim)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 10)                2363220   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,363,341\n",
      "Trainable params: 2,363,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      " 4352/75000 [>.............................] - ETA: 1:41 - loss: 0.3531 - acc: 0.8789"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-16eda8afb1b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     batch_size=128)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 array_vals.append(\n\u001b[1;32m   2654\u001b[0m                     np.asarray(value,\n\u001b[0;32m-> 2655\u001b[0;31m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "def cross_val_keras(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_keras(model, X_test, y_test, 'Model')\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can probs ignore from here down\n",
    "\n",
    "banned_dict = {}\n",
    "\n",
    "for word in banned_words_no_punct.split(\" \"):\n",
    "    if word in banned_dict:\n",
    "        banned_dict[word] += 1\n",
    "    else:\n",
    "        banned_dict[word] = 1\n",
    "        \n",
    "print(dict(list(banned_dict.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Seriously': 52, 'The': 2331, 'pats': 6, 'havent': 245, 'won': 77}\n"
     ]
    }
   ],
   "source": [
    "not_banned_dict = {}\n",
    "\n",
    "for word in not_banned_words_no_punct.split(\" \"):\n",
    "    if word in not_banned_dict:\n",
    "        not_banned_dict[word] += 1\n",
    "    else:\n",
    "        not_banned_dict[word] = 1\n",
    "        \n",
    "print(dict(list(not_banned_dict.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'know': 39, 'that': 0, 'strenuous': 4, 'ever': 1}\n"
     ]
    }
   ],
   "source": [
    "banned_counts = {}\n",
    "for word in list(banned_dict.keys()):\n",
    "    if word in not_banned_dict:\n",
    "        banned_counts[word] = banned_dict[word] - not_banned_dict[word]\n",
    "        if banned_counts[word] < 0:\n",
    "            banned_counts[word] = 0\n",
    "    else:\n",
    "        banned_counts[word] = banned_dict[word]\n",
    "\n",
    "print(dict(list(banned_counts.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Seriously': 0, 'The': 733, 'pats': 6, 'havent': 37, 'won': 66}\n"
     ]
    }
   ],
   "source": [
    "not_banned_counts = {}\n",
    "for word in list(not_banned_dict.keys()):\n",
    "    if word in banned_dict:\n",
    "        not_banned_counts[word] = not_banned_dict[word] - banned_dict[word]\n",
    "        if not_banned_counts[word] < 0:\n",
    "            not_banned_counts[word] = 0\n",
    "    else:\n",
    "        not_banned_counts[word] = not_banned_dict[word]\n",
    "        \n",
    "print(dict(list(not_banned_counts.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the banned dictionary - how many times it shows up in the not banned dictionary\n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in banned_counts:\n",
    "            weight += banned_counts[word]\n",
    "        if word in not_banned_counts:\n",
    "            weight -= not_banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBannedCount(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the banned dictionary \n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in banned_counts:\n",
    "            weight += banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNotBannedCount(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the not banned dictionary \n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in not_banned_counts:\n",
    "            weight += not_banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "feature = []\n",
    "#labels = np.array(int)\n",
    "labels = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "feature3 = []\n",
    "count = 0\n",
    "for comment in banned_comments:\n",
    "    if count < 10000:\n",
    "        feature1.append(float(getBannedCount(comment)))\n",
    "        feature2.append(float(getNotBannedCount(comment)))\n",
    "        feature3.append(float(getCounts(comment)))\n",
    "        labels.append(float(1))\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "        #labels = np.append(labels, 1)\n",
    "        \n",
    "count = 0\n",
    "for comment in not_banned_comments:\n",
    "    if count < 10000:\n",
    "        feature1.append(float(getBannedCount(comment)))\n",
    "        feature2.append(float(getNotBannedCount(comment)))\n",
    "        feature3.append(float(getCounts(comment)))\n",
    "        labels.append(float(0))\n",
    "        #labels = np.append(labels, 0)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "feature.append(feature1)\n",
    "feature.append(feature2)\n",
    "feature.append(feature3)\n",
    "labels1 = []\n",
    "labels1.append(labels)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([np.array(xi) for xi in feature])\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.array([np.array(xi) for xi in labels1])\n",
    "type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (3, 20000)\n",
      "The shape of Y is: (1, 20000)\n",
      "I have 800 training sample!\n"
     ]
    }
   ],
   "source": [
    "shape_X = features.shape\n",
    "shape_Y = label.shape\n",
    "m = 2 * 400\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have %d training sample!' % (m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerSizes(X, Y):\n",
    "    \"\"\"\n",
    "    X -- input dataset of shape \n",
    "    Y -- labels of shape\n",
    "    \"\"\"\n",
    "    input_layer_size= X.shape[0]\n",
    "    hidden_layer_size= 4\n",
    "    output_layer_size= Y.shape[0]\n",
    "    # hardcode as 1 bc we have to \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    input_layer_size -- the size of the input layer\n",
    "    hidden_layer_size -- the size of the hidden layer\n",
    "    output_layer_size -- the size of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    return (input_layer_size, hidden_layer_size, output_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size-- size of the input layer\n",
    "    hidden_size -- size of the hidden layer\n",
    "    output_size-- size of the output layer\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)  # you can pick any seed in this case\n",
    "    \n",
    "    Weight1 = np.random.randn(hidden_size,input_size) * 0.01\n",
    "    Weight2 = np.random.randn(output_size,hidden_size) * 0.01\n",
    "    bias1 = np.zeros(shape=(hidden_size, 1))\n",
    "    bias2 = np.zeros(shape=(output_size, 1))\n",
    "    \n",
    "    parameters = {\"Weight1\": Weight1,\n",
    "                  \"bias1\": bias1,\n",
    "                  \"Weight2\": Weight2,\n",
    "                  \"bias2\": bias2}\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape \n",
    "                    b1 -- bias vector of shape \n",
    "                    W2 -- weight matrix of shape \n",
    "                    b2 -- bias vector of shape\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    X -- input data of size\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    bias1 = parameters['bias1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(Weight1,X) + bias1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(Weight2,A1) + bias2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    #Values needed in the backpropagation are stored in cache. Later, it will be given to back propagation.\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    Y -- \"true\" labels vector of shape \n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]  # number of example \n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y[0]) + np.multiply((1 - Y[0]), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    ### Remember that, if you want to use different cross-entropy loss, you need to change logprobs and cost accordingly\n",
    "    \n",
    "    cost = float(np.squeeze(cost))   \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters -- dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data \n",
    "    Y -- \"true\" labels vector \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Copy W1 and W2 from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    \n",
    "        \n",
    "    # Copy A1 and A2 from dictionary \"cache\".\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    #  calculate dW1, db1, dW2, db2. \n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(Weight2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradient = {\"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                \"dW2\": dW2,\n",
    "                \"db2\": db2}\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.5):\n",
    "    \"\"\"\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \"\"\"\n",
    "    # Copy the following parameter from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias1 = parameters['bias1']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # Copy each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    Weight1 = Weight1 - learning_rate * dW1\n",
    "    Weight2 = Weight2 - learning_rate * dW2\n",
    "    bias1 = bias1 - learning_rate * db1\n",
    "    bias2 = bias2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"Weight1\": Weight1,\n",
    "                  \"Weight2\": Weight2,\n",
    "                  \"bias1\": bias1,\n",
    "                  \"bias2\": bias2}\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "    #print(parameters)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, n_h, num_iterations = 1000, print_cost=True):\n",
    "    \"\"\"\n",
    "    X -- dataset\n",
    "    Y -- labels\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent\n",
    "    print_cost -- if True, print the cost in every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(4)\n",
    "    n_x = layerSizes(X, Y)[0]\n",
    "    n_y = layerSizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    Weight1 = parameters['Weight1']\n",
    "    bias1 = parameters['bias1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # gradient descent\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Call the Forward propagation with X, and parameters.\n",
    "        A2, cache = forward_prop(X, parameters)\n",
    "        \n",
    "        # Call the Cost function with A2, Y and parameters.\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Call Backpropagation with Inputs, parameters, cache, X and Y.\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Update gradient descent parameter with  parameters and grads and learning rate.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_prop(X,parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693369\n",
      "Cost after iteration 100: 0.691049\n",
      "Cost after iteration 200: 0.690562\n",
      "Cost after iteration 300: 0.690457\n",
      "Cost after iteration 400: 0.690391\n",
      "Cost after iteration 500: 0.690337\n",
      "Cost after iteration 600: 0.690291\n",
      "Cost after iteration 700: 0.690251\n",
      "Cost after iteration 800: 0.690216\n",
      "Cost after iteration 900: 0.690187\n",
      "Accuracy: 51%\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = model(features, label, n_h = 1, num_iterations = 1000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "# plot_decision_boundary(lambda x: predict(parameters, x.T), features, label[0])\n",
    "# plt.title(\"Decision Boundary for hidden layer size \" + str(4));\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, features)\n",
    "print ('Accuracy: %d' % float((np.dot(label,predictions.T) + np.dot(1-label,1-predictions.T))/float(label.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

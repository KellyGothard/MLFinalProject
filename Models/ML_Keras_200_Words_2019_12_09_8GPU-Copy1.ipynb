{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Always import all needed libraries in the first cell\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Reshape\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model, CustomObjectScope\n",
    "import random\n",
    "import keras.backend.tensorflow_backend\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.backend import clear_session\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "np.random.seed(1) # this sets the seed so that the runs are consistent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test percent lost: 5.63\n"
     ]
    }
   ],
   "source": [
    "dftrain_banned = pd.read_csv(\"../Data/Generated/200_words_10M_banned.csv\", delimiter=',')\n",
    "dftrain_banned.insert(0, \"banned\", 1)\n",
    "\n",
    "dftrain_notbanned = pd.read_csv(\"../Data/Generated/200_words_10M_notbanned.csv\", delimiter=',')\n",
    "dftrain_notbanned.insert(0, \"banned\", 0)\n",
    "\n",
    "dfTest = pd.read_csv(\"../Data/Generated/200_words_10M_test.csv\", delimiter=',')\n",
    "dfTest = dfTest.sample(frac=1)\n",
    "\n",
    "dfTest[\"split\"] = dfTest[\"words\"].map(lambda x: x.split(\" \"), na_action='ignore')\n",
    "dfTest[\"word_cnt\"] = dfTest[\"split\"].map(lambda x: len(x), na_action='ignore')\n",
    "print(\"Test percent lost: %.2f\" % (100*len(dfTest[dfTest[\"word_cnt\"] != 200])/ len(dfTest)))\n",
    "dfTest = dfTest[dfTest[\"word_cnt\"] == 200]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006273352489925531\n",
      "0.0050474729282013655\n",
      "0.005066576393590611\n"
     ]
    }
   ],
   "source": [
    "def group_by_subreddit(df):\n",
    "    subreddit_to_comments = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"subreddit\"] in subreddit_to_comments:\n",
    "            subreddit_to_comments[row[\"subreddit\"]] += [row[\"words\"]]\n",
    "        else:\n",
    "            subreddit_to_comments[row[\"subreddit\"]] = [row[\"words\"]]\n",
    "    return subreddit_to_comments\n",
    "    \n",
    "random.seed(42)\n",
    "unique_subreddits = list(dfTest[\"subreddit\"].value_counts().keys())\n",
    "random.shuffle(unique_subreddits)\n",
    "SUBREDDIT_CNT = len(unique_subreddits)\n",
    "\n",
    "s1 = int (1/3 * SUBREDDIT_CNT)\n",
    "s2 = int (2/3 * SUBREDDIT_CNT)\n",
    "\n",
    "validation_test = unique_subreddits[:s1]\n",
    "\n",
    "threshold_test = unique_subreddits[s1:s2]\n",
    "\n",
    "testing_test = unique_subreddits[s2:]\n",
    "\n",
    "\n",
    "dfVal = dfTest[dfTest[\"subreddit\"].isin(validation_test)]\n",
    "print(dfVal[\"banned\"].sum()/ dfVal.shape[0])\n",
    "\n",
    "\n",
    "dfThresh = dfTest[dfTest[\"subreddit\"].isin(threshold_test)]\n",
    "print(dfThresh[\"banned\"].sum()/ dfThresh.shape[0])\n",
    "threshSubreddits  = group_by_subreddit(dfThresh)\n",
    "\n",
    "dfTestingTest = dfTest[dfTest[\"subreddit\"].isin(testing_test)]\n",
    "print(dfTestingTest[\"banned\"].sum()/ dfTestingTest.shape[0])\n",
    "testingTestSubreddits  = group_by_subreddit(dfTestingTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BALANCE_RATIO = 20\n",
    "TEST_BALANCE_RATIO = 185\n",
    "TRAIN_N_COMMENTS = int(len(dftrain_banned)/3)\n",
    "TEST_N_COMMENTS = int(len(dfVal)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest_banned = dfVal[dfVal[\"banned\"]]\n",
    "dfTest_notbanned = dfVal[dfVal[\"banned\"] == False]\n",
    "\n",
    "dfTest_balanced = pd.concat([dfTest_banned.head(n=TEST_N_COMMENTS), dfTest_notbanned.head(n=TEST_BALANCE_RATIO*TEST_N_COMMENTS)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain_banned_tmp = pd.concat([dftrain_banned.head(n=TRAIN_N_COMMENTS)]*int(TRAIN_BALANCE_RATIO))\n",
    "dfTrain = pd.concat([dfTrain_banned_tmp, dftrain_notbanned.head(n=TRAIN_BALANCE_RATIO*TRAIN_N_COMMENTS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2), (50000, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain[dfTrain[\"banned\"]==1].shape, dfTrain[dfTrain[\"banned\"]==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train percent lost: 1.00\n"
     ]
    }
   ],
   "source": [
    "dfTrain[\"split\"] = dfTrain[\"words\"].apply(lambda x: x.split(\" \"))\n",
    "dfTrain[\"word_cnt\"] = dfTrain[\"split\"].apply(lambda x: len(x))\n",
    "print(\"Train percent lost: %.2f\" % (100*len(dfTrain[dfTrain[\"word_cnt\"] != 200])/ len(dfTrain)))\n",
    "dfTrain = dfTrain[dfTrain[\"word_cnt\"]== 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banned</th>\n",
       "      <th>words</th>\n",
       "      <th>split</th>\n",
       "      <th>word_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1</td>\n",
       "      <td>progress . Because they are egotistical attent...</td>\n",
       "      <td>[progress, ., Because, they, are, egotistical,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19245</th>\n",
       "      <td>0</td>\n",
       "      <td>living . Oh , the tears of unfathomable sadnes...</td>\n",
       "      <td>[living, ., Oh, ,, the, tears, of, unfathomabl...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>1</td>\n",
       "      <td>Plantinga . ] ( https : //www.youtube.com/watc...</td>\n",
       "      <td>[Plantinga, ., ], (, https, :, //www.youtube.c...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31115</th>\n",
       "      <td>0</td>\n",
       "      <td>infantry ? I guess I just went full retarded ....</td>\n",
       "      <td>[infantry, ?, I, guess, I, just, went, full, r...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2274</th>\n",
       "      <td>1</td>\n",
       "      <td>it makes me wish the world would catch on fire...</td>\n",
       "      <td>[it, makes, me, wish, the, world, would, catch...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27052</th>\n",
       "      <td>0</td>\n",
       "      <td>to score pussy . Getting laid is awesome but i...</td>\n",
       "      <td>[to, score, pussy, ., Getting, laid, is, aweso...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>1</td>\n",
       "      <td>a post about it . Good luck ! I meant mana in ...</td>\n",
       "      <td>[a, post, about, it, ., Good, luck, !, I, mean...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>of rules . **** *How to use AceStream/Sopcast ...</td>\n",
       "      <td>[of, rules, ., ****, *How, to, use, AceStream/...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>1</td>\n",
       "      <td>I did actually ! It was oddly good , haha Then...</td>\n",
       "      <td>[I, did, actually, !, It, was, oddly, good, ,,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>1</td>\n",
       "      <td>. They want us to stop wanting to be free . gu...</td>\n",
       "      <td>[., They, want, us, to, stop, wanting, to, be,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       banned                                              words  \\\n",
       "587         1  progress . Because they are egotistical attent...   \n",
       "19245       0  living . Oh , the tears of unfathomable sadnes...   \n",
       "1592        1  Plantinga . ] ( https : //www.youtube.com/watc...   \n",
       "31115       0  infantry ? I guess I just went full retarded ....   \n",
       "2274        1  it makes me wish the world would catch on fire...   \n",
       "27052       0  to score pussy . Getting laid is awesome but i...   \n",
       "2410        1  a post about it . Good luck ! I meant mana in ...   \n",
       "71          1  of rules . **** *How to use AceStream/Sopcast ...   \n",
       "1588        1  I did actually ! It was oddly good , haha Then...   \n",
       "1206        1  . They want us to stop wanting to be free . gu...   \n",
       "\n",
       "                                                   split  word_cnt  \n",
       "587    [progress, ., Because, they, are, egotistical,...       200  \n",
       "19245  [living, ., Oh, ,, the, tears, of, unfathomabl...       200  \n",
       "1592   [Plantinga, ., ], (, https, :, //www.youtube.c...       200  \n",
       "31115  [infantry, ?, I, guess, I, just, went, full, r...       200  \n",
       "2274   [it, makes, me, wish, the, world, would, catch...       200  \n",
       "27052  [to, score, pussy, ., Getting, laid, is, aweso...       200  \n",
       "2410   [a, post, about, it, ., Good, luck, !, I, mean...       200  \n",
       "71     [of, rules, ., ****, *How, to, use, AceStream/...       200  \n",
       "1588   [I, did, actually, !, It, was, oddly, good, ,,...       200  \n",
       "1206   [., They, want, us, to, stop, wanting, to, be,...       200  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = dfTrain.sample(frac=1)\n",
    "dfTrain.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CustomObjectScope({\"f1_m\": f1_m, \"cross_val\":cross_val, \"recall_m\":recall_m, \"precision_m\":precision_m}):\n",
    "    model_loaded = load_model(\"../Data/Cached/Model.h5\")\n",
    "    parallel_model = multi_gpu_model(model_loaded, gpus=8, cpu_merge=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(dfTrain[\"words\"])\n",
    "y_train = dfTrain[\"banned\"]\n",
    "\n",
    "X_test = vectorizer.transform(dfTest_balanced[\"words\"])\n",
    "y_test = dfTest_balanced[\"banned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5208b84fd6cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mto_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthreshSubreddits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_process\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mparallel_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# def predict_batch(examples):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0m_sparsetools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_todense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "to_process = flatten(threshSubreddits.values())\n",
    "X_thresh = vectorizer.transform(to_process)\n",
    "# res =  parallel_model.predict(X_thresh)\n",
    "\n",
    "# def predict_batch(examples):\n",
    "#     X_thresh = vectorizer.transform(examples)\n",
    "#     return parallel_model.predict(X_thresh, batch_size=32)\n",
    "    \n",
    "# # predict_batch(threshSubreddits[\"fakeid\"])\n",
    "# predictions = Parallel(n_jobs=-1)(delayed(predict_batch)(examples) for examples in tqdm(threshSubreddits.values()))\n",
    "# # thresh_predications = {}\n",
    "\n",
    "# for subreddit, examples in tqdm(threshSubreddits.items()):\n",
    "#     X_thresh = vectorizer.transform(examples)\n",
    "#     print(len(examples), len(threshSubreddits), flush=True)    \n",
    "#     thresh_predications[subreddit] = model_loaded.predict(X_thresh, batch_size=len(examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/67 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏         | 1/67 [00:04<05:25,  4.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 [6.2388445e-08, 2.2498864e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 2/67 [00:09<05:18,  4.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 [3.8185553e-06, 6.23929e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 3/67 [00:14<05:13,  4.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 [3.0766267e-05, 2.2055249e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  6%|▌         | 4/67 [00:19<05:07,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 [2.3289444e-05, 2.4484973e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  7%|▋         | 5/67 [00:24<05:02,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 [8.261579e-08, 1.1936933e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  9%|▉         | 6/67 [00:29<04:56,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 [1.5864599e-07, 2.2449098e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 7/67 [00:34<04:51,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000 [6.0419516e-05, 5.608931e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 12%|█▏        | 8/67 [00:38<04:46,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 [4.4131084e-06, 2.2996174e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 13%|█▎        | 9/67 [00:43<04:43,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 [4.1631847e-06, 1.5849507e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▍        | 10/67 [00:48<04:37,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 [2.2286678e-05, 8.928304e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 16%|█▋        | 11/67 [00:53<04:32,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000 [2.260762e-06, 1.6650834e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 18%|█▊        | 12/67 [00:58<04:27,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 [6.491989e-08, 4.1730766e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 19%|█▉        | 13/67 [01:03<04:21,  4.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26000 [1.7642032e-08, 1.3401085e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 21%|██        | 14/67 [01:08<04:17,  4.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000 [1.36541e-07, 2.8662635e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 22%|██▏       | 15/67 [01:12<04:12,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 [3.1157728e-05, 5.729537e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 24%|██▍       | 16/67 [01:17<04:08,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 [1.9023435e-07, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 17/67 [01:22<04:04,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34000 [6.405722e-07, 2.250961e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 27%|██▋       | 18/67 [01:27<03:59,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36000 [3.4464398e-07, 1.8786941e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 28%|██▊       | 19/67 [01:32<03:55,  4.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38000 [7.4968325e-06, 1.8727482e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|██▉       | 20/67 [01:37<03:49,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 [3.088338e-05, 2.2790452e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 31%|███▏      | 21/67 [01:42<03:45,  4.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000 [0.0, 1.3064245e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 22/67 [01:47<03:39,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44000 [0.00019085301, 1.9753486e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▍      | 23/67 [01:52<03:35,  4.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000 [1.1030998e-05, 1.2822302e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 36%|███▌      | 24/67 [01:57<03:30,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 [1.12351945e-05, 5.311288e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 37%|███▋      | 25/67 [02:01<03:25,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 [0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 39%|███▉      | 26/67 [02:06<03:20,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52000 [1.6008404e-07, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 27/67 [02:11<03:14,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000 [0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 42%|████▏     | 28/67 [02:16<03:08,  4.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000 [1.2126472e-06, 3.069082e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 43%|████▎     | 29/67 [02:21<03:04,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58000 [5.300962e-08, 5.62134e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▍     | 30/67 [02:26<03:00,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 [5.9422164e-06, 1.8427751e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 46%|████▋     | 31/67 [02:31<02:55,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62000 [5.5490915e-08, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 48%|████▊     | 32/67 [02:35<02:50,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000 [0.0, 1.873241e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 49%|████▉     | 33/67 [02:40<02:46,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66000 [5.844972e-07, 2.0999346e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 51%|█████     | 34/67 [02:45<02:40,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68000 [5.16436e-06, 1.7929493e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 52%|█████▏    | 35/67 [02:50<02:35,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000 [1.2771267e-07, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 54%|█████▎    | 36/67 [02:55<02:31,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72000 [0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 37/67 [03:00<02:25,  4.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74000 [2.4384265e-06, 6.63608e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 57%|█████▋    | 38/67 [03:05<02:21,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76000 [1.14899606e-07, 3.340644e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 58%|█████▊    | 39/67 [03:10<02:16,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78000 [1.5447976e-06, 2.3248554e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|█████▉    | 40/67 [03:14<02:11,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000 [5.389272e-06, 1.4382489e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 61%|██████    | 41/67 [03:19<02:06,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82000 [1.8294671e-06, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 63%|██████▎   | 42/67 [03:24<02:01,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000 [8.719198e-05, 4.5105534e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 64%|██████▍   | 43/67 [03:29<01:55,  4.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86000 [0.5511485, 3.845357e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 66%|██████▌   | 44/67 [03:34<01:52,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88000 [0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 45/67 [03:39<01:47,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000 [1.135762e-06, 5.0175726e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 69%|██████▊   | 46/67 [03:44<01:42,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92000 [0.0, 2.8610676e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 47/67 [03:48<01:37,  4.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94000 [0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 72%|███████▏  | 48/67 [03:53<01:32,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000 [3.9188013e-05, 3.2721517e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 73%|███████▎  | 49/67 [03:58<01:27,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98000 [1.0041856e-06, 2.011623e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▍  | 50/67 [04:03<01:22,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 [0.00010256299, 1.9254561e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 76%|███████▌  | 51/67 [04:08<01:17,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102000 [7.0147144e-07, 1.8768245e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 78%|███████▊  | 52/67 [04:13<01:13,  4.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104000 [0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 79%|███████▉  | 53/67 [04:18<01:08,  4.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106000 [7.192356e-06, 1.1808016e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 81%|████████  | 54/67 [04:23<01:03,  4.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108000 [2.2998272e-08, 2.798435e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 82%|████████▏ | 55/67 [04:28<00:58,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110000 [1.9328292e-08, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 84%|████████▎ | 56/67 [04:32<00:53,  4.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112000 [4.585826e-06, 2.2221451e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▌ | 57/67 [04:37<00:48,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114000 [7.03698e-08, 1.4907379e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 87%|████████▋ | 58/67 [04:42<00:43,  4.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116000 [0.0, 1.4926043e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 88%|████████▊ | 59/67 [04:47<00:38,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118000 [3.600654e-07, 6.640317e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|████████▉ | 60/67 [04:52<00:34,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000 [1.0161781e-07, 6.91381e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 91%|█████████ | 61/67 [04:57<00:29,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122000 [1.3672842e-06, 2.3743057e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 93%|█████████▎| 62/67 [05:02<00:24,  4.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124000 [1.7479457e-05, 2.8519962e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 94%|█████████▍| 63/67 [05:06<00:19,  4.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126000 [2.475673e-07, 3.1300928e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 96%|█████████▌| 64/67 [05:11<00:14,  4.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 [6.510458e-07, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 97%|█████████▋| 65/67 [05:16<00:09,  4.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130000 [1.1676733e-07, 1.887073e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 99%|█████████▊| 66/67 [05:21<00:04,  4.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132000 [3.5820494e-07, 5.213222e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 67/67 [05:26<00:00,  4.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134000 [2.46825e-06, 2.9163455e-06]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i in tqdm(range(int(len(to_process)/2000))):\n",
    "    preds += list(parallel_model.predict(X_thresh[i*2000:(i+1)*2000]).flatten())\n",
    "    print(len(preds), preds[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_to_preds = {}\n",
    "i = 0\n",
    "for subreddit, examples in threshSubreddits.items():\n",
    "    subreddit_to_preds[subreddit] = preds[i:i+len(examples)]\n",
    "    i += len(examples)\n",
    "#     subreddit_keys.append([subreddit] * len(examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.9918559794256322\n"
     ]
    }
   ],
   "source": [
    "# for subreddit, examples in threshSubreddits.items():\n",
    "#     print(len(examples), len(subreddit_to_preds[subreddit]))\n",
    "def read_banned_subreddits(filename):\n",
    "    \"\"\"read in banned subreddits file with each line as 'r/asfasdf' \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        banned = set()\n",
    "        for line in f:\n",
    "            if line.strip() != \"\":\n",
    "                banned.update([line.strip().split(\"r/\")[1]])\n",
    "        return banned\n",
    "\n",
    "banned_subreddits = read_banned_subreddits(\"../Data/banned-subreddits.txt\")\n",
    "    \n",
    "subreddit_to_avg = {}\n",
    "for subreddit, preds in subreddit_to_preds.items():\n",
    "    if preds:\n",
    "        subreddit_to_avg[subreddit] = np.mean(preds)\n",
    "    \n",
    "banned_means = [(mean, \"banned\") for subreddit, mean in subreddit_to_avg.items() \n",
    "                if subreddit in banned_subreddits]\n",
    "notbanned_means = [(mean, \"notbanned\") for subreddit, mean in subreddit_to_avg.items() \n",
    "                if subreddit not in banned_subreddits]\n",
    "\n",
    "svm_input = pd.DataFrame(banned_means + notbanned_means, columns=[\"mean\", \"isbanned\"])\n",
    "\n",
    "svm_model = SVC(gamma=\"scale\")\n",
    "\n",
    "svm_model.fit([[mean] for mean in svm_input[\"mean\"]], svm_input[\"isbanned\"])\n",
    "svm_model.__dict__\n",
    "\n",
    "ratio = 0.17\n",
    "print(sum(np.array([x for x,y in banned_means]) > ratio) / len(banned_means))\n",
    "print(sum(np.array([x for x,y in notbanned_means]) < ratio) / len(notbanned_means))\n",
    "\n",
    "# means = np.array(sorted(subreddit_to_avg.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "print(X_train.shape)\n",
    "print(input_dim)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dense_6_input (InputLayer)      (None, 193952)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 1)            19416521    lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 1)            0           sequential_2[1][0]               \n",
      "                                                                 sequential_2[2][0]               \n",
      "                                                                 sequential_2[3][0]               \n",
      "                                                                 sequential_2[4][0]               \n",
      "                                                                 sequential_2[5][0]               \n",
      "                                                                 sequential_2[6][0]               \n",
      "                                                                 sequential_2[7][0]               \n",
      "                                                                 sequential_2[8][0]               \n",
      "==================================================================================================\n",
      "Total params: 19,416,521\n",
      "Trainable params: 19,416,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=8, cpu_merge=False)\n",
    "parallel_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "parallel_model.summary()\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['acc',f1_m,precision_m, recall_m])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98995 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "98995/98995 [==============================] - 145s 1ms/step - loss: 0.2650 - acc: 0.8764 - f1_m: 0.7945 - precision_m: 0.9613 - recall_m: 0.7653 - val_loss: 0.0439 - val_acc: 0.9907 - val_f1_m: 0.2823 - val_precision_m: 0.2540 - val_recall_m: 0.3742\n",
      "Epoch 2/2\n",
      "98995/98995 [==============================] - 140s 1ms/step - loss: 0.0068 - acc: 0.9988 - f1_m: 0.9987 - precision_m: 0.9979 - recall_m: 0.9996 - val_loss: 0.0515 - val_acc: 0.9911 - val_f1_m: 0.3076 - val_precision_m: 0.2823 - val_recall_m: 0.4510\n"
     ]
    }
   ],
   "source": [
    "history = parallel_model.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test[:10000], y_test[:10000]),\n",
    "                    batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model.save(\"../Data/Cached/Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CustomObjectScope({\"f1_m\": f1_m, \"cross_val\":cross_val, \"recall_m\":recall_m, \"precision_m\":precision_m}):\n",
    "    model_loaded = load_model(\"../Data/Cached/Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_thresh = vectorizer.transform(flatten(threshSubreddits.values()))\n",
    "\n",
    "def predict_batch(examples):\n",
    "    X_thresh = vectorizer.transform(examples)\n",
    "    return parallel_model.predict(X_thresh, batch_size=256)\n",
    "    \n",
    "    \n",
    "predictions = Parallel(n_jobs=-1)(delayed(predict_batch)(examples) for examples in tqdm(threshSubreddits.values()))\n",
    "\n",
    "# for subreddit, examples in threshSubreddits.items():\n",
    "#     X_thresh = vectorizer.transform(examples)\n",
    "#     print(len(examples), flush=True)\n",
    "#     thresh_predications[subreddit] = parallel_model.predict(X_thresh, batch_size=len(examples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_uses_inputs_arg': True,\n",
       " 'inputs': [<tf.Tensor 'dense_6_input:0' shape=(?, 193952) dtype=float32>],\n",
       " 'outputs': [<tf.Tensor 'dense_10_1/concat:0' shape=(?, 1) dtype=float32>],\n",
       " 'name': 'model_2',\n",
       " 'trainable': True,\n",
       " '_is_compiled': True,\n",
       " '_expects_training_arg': False,\n",
       " '_initial_weights': None,\n",
       " 'supports_masking': False,\n",
       " 'optimizer': <keras.optimizers.Adam at 0x2b5b6e8ec278>,\n",
       " '_updates': [],\n",
       " '_losses': [],\n",
       " '_per_input_losses': {},\n",
       " '_per_input_updates': {},\n",
       " '_layers': [<keras.engine.input_layer.InputLayer at 0x2b5e26dce668>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e26c66240>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e26f361d0>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e26f36198>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e26ffa470>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e270189e8>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e27218240>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e272f5da0>,\n",
       "  <keras.layers.core.Lambda at 0x2b5e272f5dd8>,\n",
       "  <keras.engine.sequential.Sequential at 0x2b5e26dce400>,\n",
       "  <keras.layers.merge.Concatenate at 0x2b5e26c66048>],\n",
       " '_outbound_nodes': [],\n",
       " '_inbound_nodes': [<keras.engine.base_layer.Node at 0x2b5e26c66278>],\n",
       " '_compute_previous_mask': True,\n",
       " '_built': True,\n",
       " '_is_graph_network': True,\n",
       " '_input_layers': [<keras.engine.input_layer.InputLayer at 0x2b5e26dce668>],\n",
       " '_output_layers': [<keras.layers.merge.Concatenate at 0x2b5e26c66048>],\n",
       " '_input_coordinates': [(<keras.engine.input_layer.InputLayer at 0x2b5e26dce668>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_coordinates': [(<keras.layers.merge.Concatenate at 0x2b5e26c66048>,\n",
       "   0,\n",
       "   0)],\n",
       " '_output_mask_cache': {'47683378932144_94594081472752': None},\n",
       " '_output_tensor_cache': {},\n",
       " '_output_shape_cache': {},\n",
       " '_network_nodes': {'dense_10_ib-0',\n",
       "  'dense_6_input_ib-0',\n",
       "  'lambda_10_ib-0',\n",
       "  'lambda_11_ib-0',\n",
       "  'lambda_12_ib-0',\n",
       "  'lambda_13_ib-0',\n",
       "  'lambda_14_ib-0',\n",
       "  'lambda_15_ib-0',\n",
       "  'lambda_16_ib-0',\n",
       "  'lambda_9_ib-0',\n",
       "  'sequential_2_ib-1',\n",
       "  'sequential_2_ib-2',\n",
       "  'sequential_2_ib-3',\n",
       "  'sequential_2_ib-4',\n",
       "  'sequential_2_ib-5',\n",
       "  'sequential_2_ib-6',\n",
       "  'sequential_2_ib-7',\n",
       "  'sequential_2_ib-8'},\n",
       " '_nodes_by_depth': {0: [<keras.engine.base_layer.Node at 0x2b5e274d7a20>],\n",
       "  1: [<keras.engine.base_layer.Node at 0x2b5e26c66e48>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e26f36208>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e26f36080>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e27018a20>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e272dc588>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e271f7908>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e274bfb00>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e273ef9e8>],\n",
       "  2: [<keras.engine.base_layer.Node at 0x2b5e275c1748>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e274d7c50>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e273de780>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e272f5d30>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e271dd748>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e270d9b38>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e26ffa780>,\n",
       "   <keras.engine.base_layer.Node at 0x2b5e26f36da0>],\n",
       "  3: [<keras.engine.base_layer.Node at 0x2b5e26dce6a0>]},\n",
       " '_layers_by_depth': {0: [<keras.layers.merge.Concatenate at 0x2b5e26c66048>],\n",
       "  1: [<keras.engine.sequential.Sequential at 0x2b5e26dce400>],\n",
       "  2: [<keras.layers.core.Lambda at 0x2b5e26c66240>,\n",
       "   <keras.layers.core.Lambda at 0x2b5e26f361d0>,\n",
       "   <keras.layers.core.Lambda at 0x2b5e26f36198>,\n",
       "   <keras.layers.core.Lambda at 0x2b5e26ffa470>,\n",
       "   <keras.layers.core.Lambda at 0x2b5e270189e8>,\n",
       "   <keras.layers.core.Lambda at 0x2b5e27218240>,\n",
       "   <keras.layers.core.Lambda at 0x2b5e272f5da0>,\n",
       "   <keras.layers.core.Lambda at 0x2b5e272f5dd8>],\n",
       "  3: [<keras.engine.input_layer.InputLayer at 0x2b5e26dce668>]},\n",
       " 'input_names': ['dense_6_input'],\n",
       " 'output_names': ['dense_10'],\n",
       " '_feed_input_names': ['dense_6_input'],\n",
       " '_feed_inputs': [<tf.Tensor 'dense_6_input:0' shape=(?, 193952) dtype=float32>],\n",
       " '_feed_input_shapes': [(None, 193952)],\n",
       " 'loss': 'binary_crossentropy',\n",
       " 'metrics': ['acc',\n",
       "  <function __main__.f1_m(y_true, y_pred)>,\n",
       "  <function __main__.precision_m(y_true, y_pred)>,\n",
       "  <function __main__.recall_m(y_true, y_pred)>],\n",
       " 'loss_weights': None,\n",
       " 'sample_weight_mode': None,\n",
       " 'weighted_metrics': None,\n",
       " 'loss_functions': [<function keras.losses.binary_crossentropy(y_true, y_pred)>],\n",
       " '_feed_outputs': [<tf.Tensor 'dense_10_1/concat:0' shape=(?, 1) dtype=float32>],\n",
       " '_feed_output_names': ['dense_10'],\n",
       " '_feed_output_shapes': [(None, 1)],\n",
       " '_feed_loss_fns': [<function keras.losses.binary_crossentropy(y_true, y_pred)>],\n",
       " 'targets': [<tf.Tensor 'dense_10_target:0' shape=(?, ?) dtype=float32>],\n",
       " '_feed_targets': [<tf.Tensor 'dense_10_target:0' shape=(?, ?) dtype=float32>],\n",
       " 'sample_weight_modes': [None],\n",
       " '_feed_sample_weight_modes': [None],\n",
       " 'metrics_names': ['loss', 'acc', 'f1_m', 'precision_m', 'recall_m'],\n",
       " 'metrics_tensors': [<tf.Tensor 'metrics_1/acc/Mean_1:0' shape=() dtype=float32>,\n",
       "  <tf.Tensor 'metrics_1/f1_m/Mean:0' shape=() dtype=float32>,\n",
       "  <tf.Tensor 'metrics_1/precision_m/Mean:0' shape=() dtype=float32>,\n",
       "  <tf.Tensor 'metrics_1/recall_m/Mean:0' shape=() dtype=float32>],\n",
       " 'metrics_updates': [],\n",
       " 'stateful_metric_names': [],\n",
       " 'stateful_metric_functions': [],\n",
       " 'total_loss': <tf.Tensor 'loss_1/mul:0' shape=() dtype=float32>,\n",
       " 'sample_weights': [<tf.Tensor 'dense_10_sample_weights:0' shape=(?,) dtype=float32>],\n",
       " '_feed_sample_weights': [<tf.Tensor 'dense_10_sample_weights:0' shape=(?,) dtype=float32>],\n",
       " '_function_kwargs': {},\n",
       " 'train_function': <keras.backend.tensorflow_backend.Function at 0x2b5e278202b0>,\n",
       " 'test_function': <keras.backend.tensorflow_backend.Function at 0x2b5e26c660f0>,\n",
       " 'predict_function': <keras.backend.tensorflow_backend.Function at 0x2b665e1e7da0>,\n",
       " '_collected_trainable_weights': [<tf.Variable 'dense_6/kernel:0' shape=(193952, 100) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_6/bias:0' shape=(100,) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_7/kernel:0' shape=(100, 100) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_7/bias:0' shape=(100,) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_8/kernel:0' shape=(100, 100) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_8/bias:0' shape=(100,) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_9/kernel:0' shape=(100, 10) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_9/bias:0' shape=(10,) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_10/kernel:0' shape=(10, 1) dtype=float32_ref>,\n",
       "  <tf.Variable 'dense_10/bias:0' shape=(1,) dtype=float32_ref>],\n",
       " 'history': <keras.callbacks.History at 0x2b5e278203c8>,\n",
       " 'stop_training': False}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'use_multiprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-7b52c6a30bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'use_multiprocessing'"
     ]
    }
   ],
   "source": [
    "prediction = model_loaded.predict(X_thresh, use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

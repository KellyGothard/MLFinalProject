{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Always import all needed libraries in the first cell\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Reshape\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "\n",
    "np.random.seed(1) # this sets the seed so that the runs are consistent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test percent lost: 5.63\n"
     ]
    }
   ],
   "source": [
    "dftrain_banned = pd.read_csv(\"../Data/Generated/200_words_10M_banned.csv\", delimiter=',')\n",
    "dftrain_banned.insert(0, \"banned\", 1)\n",
    "\n",
    "dftrain_notbanned = pd.read_csv(\"../Data/Generated/200_words_10M_notbanned.csv\", delimiter=',')\n",
    "dftrain_notbanned.insert(0, \"banned\", 0)\n",
    "\n",
    "dfTest = pd.read_csv(\"../Data/Generated/200_words_10M_test.csv\", delimiter=',')\n",
    "dfTest = dfTest.sample(frac=1)\n",
    "\n",
    "dfTest[\"split\"] = dfTest[\"words\"].map(lambda x: x.split(\" \"), na_action='ignore')\n",
    "dfTest[\"word_cnt\"] = dfTest[\"split\"].map(lambda x: len(x), na_action='ignore')\n",
    "print(\"Test percent lost: %.2f\" % (100*len(dfTest[dfTest[\"word_cnt\"] != 200])/ len(dfTest)))\n",
    "dfTest = dfTest[dfTest[\"word_cnt\"] == 200]\n",
    "\n",
    "dfTest_banned = dfTest[dfTest[\"banned\"]]\n",
    "dfTest_notbanned = dfTest[dfTest[\"banned\"] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BALANCE_RATIO = 10\n",
    "TEST_BALANCE_RATIO = 1\n",
    "TRAIN_N_COMMENTS = int(len(dftrain_banned)/1)\n",
    "TEST_N_COMMENTS = int(len(dfTest_banned)/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest_balanced = pd.concat([dfTest_banned.head(n=TEST_N_COMMENTS), dfTest_notbanned.head(n=TEST_BALANCE_RATIO*TEST_N_COMMENTS)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = pd.concat([dftrain_banned.head(n=TRAIN_N_COMMENTS), dftrain_notbanned.head(n=TRAIN_BALANCE_RATIO*TRAIN_N_COMMENTS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500, 2), (75000, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain[dfTrain[\"banned\"]==1].shape, dfTrain[dfTrain[\"banned\"]==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train percent lost: 0.20\n"
     ]
    }
   ],
   "source": [
    "dfTrain[\"split\"] = dfTrain[\"words\"].apply(lambda x: x.split(\" \"))\n",
    "dfTrain[\"word_cnt\"] = dfTrain[\"split\"].apply(lambda x: len(x))\n",
    "print(\"Train percent lost: %.2f\" % (100*len(dfTrain[dfTrain[\"word_cnt\"] != 200])/ len(dfTrain)))\n",
    "dfTrain = dfTrain[dfTrain[\"word_cnt\"]== 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banned</th>\n",
       "      <th>words</th>\n",
       "      <th>split</th>\n",
       "      <th>word_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12373</th>\n",
       "      <td>0</td>\n",
       "      <td>low , and as you said , these risks can eventu...</td>\n",
       "      <td>[low, ,, and, as, you, said, ,, these, risks, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17874</th>\n",
       "      <td>0</td>\n",
       "      <td>144 FPS all the time . I play a lot of esports...</td>\n",
       "      <td>[144, FPS, all, the, time, ., I, play, a, lot,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17423</th>\n",
       "      <td>0</td>\n",
       "      <td>( http : //i.imgur.com/NRwJVzR.gifv ) I 'm thi...</td>\n",
       "      <td>[(, http, :, //i.imgur.com/NRwJVzR.gifv, ), I,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40125</th>\n",
       "      <td>0</td>\n",
       "      <td>the cap on . tailgating on the lake would be s...</td>\n",
       "      <td>[the, cap, on, ., tailgating, on, the, lake, w...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54495</th>\n",
       "      <td>0</td>\n",
       "      <td>you could ’ ve quickly gone for E and use that...</td>\n",
       "      <td>[you, could, ’, ve, quickly, gone, for, E, and...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18073</th>\n",
       "      <td>0</td>\n",
       "      <td>ahead . The Pepsi GMT ( 16710 I think ) is my ...</td>\n",
       "      <td>[ahead, ., The, Pepsi, GMT, (, 16710, I, think...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5964</th>\n",
       "      <td>1</td>\n",
       "      <td>and I encountered it consistently in the south...</td>\n",
       "      <td>[and, I, encountered, it, consistently, in, th...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53972</th>\n",
       "      <td>0</td>\n",
       "      <td>campaign . Also , how is `` Strong Leadership ...</td>\n",
       "      <td>[campaign, ., Also, ,, how, is, ``, Strong, Le...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60562</th>\n",
       "      <td>0</td>\n",
       "      <td>thankful as hell to have the team and staff th...</td>\n",
       "      <td>[thankful, as, hell, to, have, the, team, and,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44734</th>\n",
       "      <td>0</td>\n",
       "      <td>'ll have to try that . I 've been using compre...</td>\n",
       "      <td>['ll, have, to, try, that, ., I, 've, been, us...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       banned                                              words  \\\n",
       "12373       0  low , and as you said , these risks can eventu...   \n",
       "17874       0  144 FPS all the time . I play a lot of esports...   \n",
       "17423       0  ( http : //i.imgur.com/NRwJVzR.gifv ) I 'm thi...   \n",
       "40125       0  the cap on . tailgating on the lake would be s...   \n",
       "54495       0  you could ’ ve quickly gone for E and use that...   \n",
       "18073       0  ahead . The Pepsi GMT ( 16710 I think ) is my ...   \n",
       "5964        1  and I encountered it consistently in the south...   \n",
       "53972       0  campaign . Also , how is `` Strong Leadership ...   \n",
       "60562       0  thankful as hell to have the team and staff th...   \n",
       "44734       0  'll have to try that . I 've been using compre...   \n",
       "\n",
       "                                                   split  word_cnt  \n",
       "12373  [low, ,, and, as, you, said, ,, these, risks, ...       200  \n",
       "17874  [144, FPS, all, the, time, ., I, play, a, lot,...       200  \n",
       "17423  [(, http, :, //i.imgur.com/NRwJVzR.gifv, ), I,...       200  \n",
       "40125  [the, cap, on, ., tailgating, on, the, lake, w...       200  \n",
       "54495  [you, could, ’, ve, quickly, gone, for, E, and...       200  \n",
       "18073  [ahead, ., The, Pepsi, GMT, (, 16710, I, think...       200  \n",
       "5964   [and, I, encountered, it, consistently, in, th...       200  \n",
       "53972  [campaign, ., Also, ,, how, is, ``, Strong, Le...       200  \n",
       "60562  [thankful, as, hell, to, have, the, team, and,...       200  \n",
       "44734  ['ll, have, to, try, that, ., I, 've, been, us...       200  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = dfTrain.sample(frac=1)\n",
    "dfTrain.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(dfTrain[\"words\"])\n",
    "y_train = dfTrain[\"banned\"]\n",
    "\n",
    "X_test = vectorizer.transform(dfTest_balanced[\"words\"])\n",
    "y_test = dfTest_balanced[\"banned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 257222), matrix([[0., 0., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].todense().shape, X_train[0].todense()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82337, 257222)\n",
      "257222\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "print(X_train.shape)\n",
    "print(input_dim)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=5, activation=\"relu\", input_shape = (None, X_train[0].shape[1])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=7, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=15, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=15, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# model.add(layers.Dense(30, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "# model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# # model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-2-9eead89c3a2c>:45: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1d_21_input (InputLayer)    (None, None, 257222) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, None, 257222) 0           conv1d_21_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, None, 1)      41572129    lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Concatenate)           (None, None, 1)      0           sequential_12[1][0]              \n",
      "                                                                 sequential_12[2][0]              \n",
      "                                                                 sequential_12[3][0]              \n",
      "                                                                 sequential_12[4][0]              \n",
      "                                                                 sequential_12[5][0]              \n",
      "                                                                 sequential_12[6][0]              \n",
      "                                                                 sequential_12[7][0]              \n",
      "                                                                 sequential_12[8][0]              \n",
      "==================================================================================================\n",
      "Total params: 41,572,129\n",
      "Trainable params: 41,572,129\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=8, cpu_merge=False)\n",
    "\n",
    "parallel_model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)],\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',f1_m,precision_m, recall_m] )\n",
    "\n",
    "callbacks = [ModelCheckpoint(\"../Data/Models/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor=\"val_loss\", save_best_only=True),\n",
    "                        EarlyStopping(monitor=\"val_loss\", min_delta = 0, patience=3)]\n",
    "parallel_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv1d_21_input to have 3 dimensions, but got array with shape (82337, 257222)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2f8a565d7c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     batch_size=1024)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_21_input to have 3 dimensions, but got array with shape (82337, 257222)"
     ]
    }
   ],
   "source": [
    "history = parallel_model.fit(X_train.reshape(X_train.shape[0], 1, X_train.shape[1]), y_train,\n",
    "                    epochs=100,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test[:10000], y_test[:10000]),\n",
    "                    callbacks = callbacks,\n",
    "                    batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "def cross_val_keras(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_keras(model, X_test, y_test, 'Model')\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Can probs ignore from here down\n",
    "\n",
    "banned_dict = {}\n",
    "\n",
    "for word in banned_words_no_punct.split(\" \"):\n",
    "    if word in banned_dict:\n",
    "        banned_dict[word] += 1\n",
    "    else:\n",
    "        banned_dict[word] = 1\n",
    "        \n",
    "print(dict(list(banned_dict.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Seriously': 52, 'The': 2331, 'pats': 6, 'havent': 245, 'won': 77}\n"
     ]
    }
   ],
   "source": [
    "not_banned_dict = {}\n",
    "\n",
    "for word in not_banned_words_no_punct.split(\" \"):\n",
    "    if word in not_banned_dict:\n",
    "        not_banned_dict[word] += 1\n",
    "    else:\n",
    "        not_banned_dict[word] = 1\n",
    "        \n",
    "print(dict(list(not_banned_dict.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'know': 39, 'that': 0, 'strenuous': 4, 'ever': 1}\n"
     ]
    }
   ],
   "source": [
    "banned_counts = {}\n",
    "for word in list(banned_dict.keys()):\n",
    "    if word in not_banned_dict:\n",
    "        banned_counts[word] = banned_dict[word] - not_banned_dict[word]\n",
    "        if banned_counts[word] < 0:\n",
    "            banned_counts[word] = 0\n",
    "    else:\n",
    "        banned_counts[word] = banned_dict[word]\n",
    "\n",
    "print(dict(list(banned_counts.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Seriously': 0, 'The': 733, 'pats': 6, 'havent': 37, 'won': 66}\n"
     ]
    }
   ],
   "source": [
    "not_banned_counts = {}\n",
    "for word in list(not_banned_dict.keys()):\n",
    "    if word in banned_dict:\n",
    "        not_banned_counts[word] = not_banned_dict[word] - banned_dict[word]\n",
    "        if not_banned_counts[word] < 0:\n",
    "            not_banned_counts[word] = 0\n",
    "    else:\n",
    "        not_banned_counts[word] = not_banned_dict[word]\n",
    "        \n",
    "print(dict(list(not_banned_counts.items())[0: 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCounts(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the banned dictionary - how many times it shows up in the not banned dictionary\n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in banned_counts:\n",
    "            weight += banned_counts[word]\n",
    "        if word in not_banned_counts:\n",
    "            weight -= not_banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBannedCount(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the banned dictionary \n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in banned_counts:\n",
    "            weight += banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNotBannedCount(str):\n",
    "    \"\"\"\n",
    "    get how many times the words show up in the not banned dictionary \n",
    "    \"\"\"\n",
    "    weight = 0\n",
    "    for word in str.split(\" \"):\n",
    "        if word in not_banned_counts:\n",
    "            weight += not_banned_counts[word]\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "feature = []\n",
    "#labels = np.array(int)\n",
    "labels = []\n",
    "feature1 = []\n",
    "feature2 = []\n",
    "feature3 = []\n",
    "count = 0\n",
    "for comment in banned_comments:\n",
    "    if count < 10000:\n",
    "        feature1.append(float(getBannedCount(comment)))\n",
    "        feature2.append(float(getNotBannedCount(comment)))\n",
    "        feature3.append(float(getCounts(comment)))\n",
    "        labels.append(float(1))\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "        #labels = np.append(labels, 1)\n",
    "        \n",
    "count = 0\n",
    "for comment in not_banned_comments:\n",
    "    if count < 10000:\n",
    "        feature1.append(float(getBannedCount(comment)))\n",
    "        feature2.append(float(getNotBannedCount(comment)))\n",
    "        feature3.append(float(getCounts(comment)))\n",
    "        labels.append(float(0))\n",
    "        #labels = np.append(labels, 0)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "feature.append(feature1)\n",
    "feature.append(feature2)\n",
    "feature.append(feature3)\n",
    "labels1 = []\n",
    "labels1.append(labels)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array([np.array(xi) for xi in feature])\n",
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.array([np.array(xi) for xi in labels1])\n",
    "type(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (3, 20000)\n",
      "The shape of Y is: (1, 20000)\n",
      "I have 800 training sample!\n"
     ]
    }
   ],
   "source": [
    "shape_X = features.shape\n",
    "shape_Y = label.shape\n",
    "m = 2 * 400\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have %d training sample!' % (m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerSizes(X, Y):\n",
    "    \"\"\"\n",
    "    X -- input dataset of shape \n",
    "    Y -- labels of shape\n",
    "    \"\"\"\n",
    "    input_layer_size= X.shape[0]\n",
    "    hidden_layer_size= 4\n",
    "    output_layer_size= Y.shape[0]\n",
    "    # hardcode as 1 bc we have to \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    input_layer_size -- the size of the input layer\n",
    "    hidden_layer_size -- the size of the hidden layer\n",
    "    output_layer_size -- the size of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    return (input_layer_size, hidden_layer_size, output_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size-- size of the input layer\n",
    "    hidden_size -- size of the hidden layer\n",
    "    output_size-- size of the output layer\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)  # you can pick any seed in this case\n",
    "    \n",
    "    Weight1 = np.random.randn(hidden_size,input_size) * 0.01\n",
    "    Weight2 = np.random.randn(output_size,hidden_size) * 0.01\n",
    "    bias1 = np.zeros(shape=(hidden_size, 1))\n",
    "    bias2 = np.zeros(shape=(output_size, 1))\n",
    "    \n",
    "    parameters = {\"Weight1\": Weight1,\n",
    "                  \"bias1\": bias1,\n",
    "                  \"Weight2\": Weight2,\n",
    "                  \"bias2\": bias2}\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape \n",
    "                    b1 -- bias vector of shape \n",
    "                    W2 -- weight matrix of shape \n",
    "                    b2 -- bias vector of shape\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "    \"\"\"\n",
    "    X -- input data of size\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    bias1 = parameters['bias1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    Z1 = np.dot(Weight1,X) + bias1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(Weight2,A1) + bias2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    #Values needed in the backpropagation are stored in cache. Later, it will be given to back propagation.\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    Y -- \"true\" labels vector of shape \n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]  # number of example \n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    logprobs = np.multiply(np.log(A2), Y[0]) + np.multiply((1 - Y[0]), np.log(1 - A2))\n",
    "    cost = - np.sum(logprobs) / m\n",
    "    \n",
    "    ### Remember that, if you want to use different cross-entropy loss, you need to change logprobs and cost accordingly\n",
    "    \n",
    "    cost = float(np.squeeze(cost))   \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters -- dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data \n",
    "    Y -- \"true\" labels vector \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Copy W1 and W2 from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    \n",
    "        \n",
    "    # Copy A1 and A2 from dictionary \"cache\".\n",
    "    \n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    #  calculate dW1, db1, dW2, db2. \n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(Weight2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradient = {\"dW1\": dW1,\n",
    "                \"db1\": db1,\n",
    "                \"dW2\": dW2,\n",
    "                \"db2\": db2}\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.5):\n",
    "    \"\"\"\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \"\"\"\n",
    "    # Copy the following parameter from the dictionary \"parameters\"\n",
    "    Weight1 = parameters['Weight1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias1 = parameters['bias1']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # Copy each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2']\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    Weight1 = Weight1 - learning_rate * dW1\n",
    "    Weight2 = Weight2 - learning_rate * dW2\n",
    "    bias1 = bias1 - learning_rate * db1\n",
    "    bias2 = bias2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"Weight1\": Weight1,\n",
    "                  \"Weight2\": Weight2,\n",
    "                  \"bias1\": bias1,\n",
    "                  \"bias2\": bias2}\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "    \"\"\"\n",
    "    #print(parameters)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, n_h, num_iterations = 1000, print_cost=True):\n",
    "    \"\"\"\n",
    "    X -- dataset\n",
    "    Y -- labels\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent\n",
    "    print_cost -- if True, print the cost in every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(4)\n",
    "    n_x = layerSizes(X, Y)[0]\n",
    "    n_y = layerSizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    Weight1 = parameters['Weight1']\n",
    "    bias1 = parameters['bias1']\n",
    "    Weight2 = parameters['Weight2']\n",
    "    bias2 = parameters['bias2']\n",
    "    \n",
    "    # gradient descent\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Call the Forward propagation with X, and parameters.\n",
    "        A2, cache = forward_prop(X, parameters)\n",
    "        \n",
    "        # Call the Cost function with A2, Y and parameters.\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Call Backpropagation with Inputs, parameters, cache, X and Y.\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Update gradient descent parameter with  parameters and grads and learning rate.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    A2, cache = forward_prop(X,parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693369\n",
      "Cost after iteration 100: 0.691049\n",
      "Cost after iteration 200: 0.690562\n",
      "Cost after iteration 300: 0.690457\n",
      "Cost after iteration 400: 0.690391\n",
      "Cost after iteration 500: 0.690337\n",
      "Cost after iteration 600: 0.690291\n",
      "Cost after iteration 700: 0.690251\n",
      "Cost after iteration 800: 0.690216\n",
      "Cost after iteration 900: 0.690187\n",
      "Accuracy: 51%\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = model(features, label, n_h = 1, num_iterations = 1000, print_cost=True)\n",
    "\n",
    "# Plot the decision boundary\n",
    "# plot_decision_boundary(lambda x: predict(parameters, x.T), features, label[0])\n",
    "# plt.title(\"Decision Boundary for hidden layer size \" + str(4));\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, features)\n",
    "print ('Accuracy: %d' % float((np.dot(label,predictions.T) + np.dot(1-label,1-predictions.T))/float(label.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

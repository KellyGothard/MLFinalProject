{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Reshape, Conv1D, Embedding, MaxPooling1D, Dropout, Flatten, Dense, Bidirectional, LSTM\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.ConfigProto().gpu_options.allow_growth = True\n",
    "np.random.seed(1) # this sets the seed so that the runs are consistent\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.log_device_placement = True\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading w2v\n",
      "loaded from pickle\n",
      "Done loading w2v\n"
     ]
    }
   ],
   "source": [
    "# Load W2V\n",
    "W2V_Pickle = \"../Data/Cached/w2v.p\"\n",
    "print(\"loading w2v\")\n",
    "try:\n",
    "    w2v_model = pickle.load(open(W2V_Pickle, \"rb\"))\n",
    "    print(\"loaded from pickle\")\n",
    "except:\n",
    "    w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../Data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    pickle.dump(w2v_model, open(W2V_Pickle, \"wb\"))\n",
    "    print(\"loaded from model file\")\n",
    "\n",
    "print(\"Done loading w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        epsilon = K.epsilon()\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n",
    "        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n",
    "               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test percent lost: 5.63\n"
     ]
    }
   ],
   "source": [
    "dftrain_banned = pd.read_csv(\"../Data/Generated/200_words_10M_banned.csv\", delimiter=',')\n",
    "dftrain_banned.insert(0, \"banned\", 1)\n",
    "\n",
    "dftrain_notbanned = pd.read_csv(\"../Data/Generated/200_words_10M_notbanned.csv\", delimiter=',')\n",
    "dftrain_notbanned.insert(0, \"banned\", 0)\n",
    "\n",
    "dfTest = pd.read_csv(\"../Data/Generated/200_words_10M_test.csv\", delimiter=',')\n",
    "dfTest = dfTest.sample(frac=1)\n",
    "\n",
    "dfTest[\"split\"] = dfTest[\"words\"].map(lambda x: x.split(\" \"), na_action='ignore')\n",
    "dfTest[\"word_cnt\"] = dfTest[\"split\"].map(lambda x: len(x), na_action='ignore')\n",
    "print(\"Test percent lost: %.2f\" % (100*len(dfTest[dfTest[\"word_cnt\"] != SEQ_LEN])/ len(dfTest)))\n",
    "dfTest = dfTest[dfTest[\"word_cnt\"] == SEQ_LEN]\n",
    "\n",
    "dfTest_banned = dfTest[dfTest[\"banned\"]]\n",
    "dfTest_notbanned = dfTest[dfTest[\"banned\"] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BALANCE_RATIO = 40\n",
    "TEST_BALANCE_RATIO = 1\n",
    "TRAIN_N_COMMENTS = int(len(dftrain_banned)/1)\n",
    "TEST_N_COMMENTS = int(len(dfTest_banned)/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7500, 2), (300000, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTest_balanced = pd.concat([dfTest_banned.head(n=TEST_N_COMMENTS), dfTest_notbanned.head(n=TEST_BALANCE_RATIO*TEST_N_COMMENTS)]).sample(frac=1)\n",
    "dfTrain = pd.concat([dftrain_banned.head(n=TRAIN_N_COMMENTS), dftrain_notbanned.head(n=TRAIN_BALANCE_RATIO*TRAIN_N_COMMENTS)])\n",
    "dfTrain[dfTrain[\"banned\"]==1].shape, dfTrain[dfTrain[\"banned\"]==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banned</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271582</th>\n",
       "      <td>0</td>\n",
       "      <td>its inaugural edition of the report 'Poverty a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72781</th>\n",
       "      <td>0</td>\n",
       "      <td>cool '' will suddenly make you unique and inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197561</th>\n",
       "      <td>0</td>\n",
       "      <td>from him . If he is in your class and sits nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70760</th>\n",
       "      <td>0</td>\n",
       "      <td>is now just a safe space nonsense zone when it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51467</th>\n",
       "      <td>0</td>\n",
       "      <td>is awesome ! Canada was there too . [ removed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49464</th>\n",
       "      <td>0</td>\n",
       "      <td>Cyberface/Medic what if they win more than 8 g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144111</th>\n",
       "      <td>0</td>\n",
       "      <td>`` Stop Crossposting My Stuff ! '' ) ] ( https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249539</th>\n",
       "      <td>0</td>\n",
       "      <td>being built in Europe for whatever that 's wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61102</th>\n",
       "      <td>0</td>\n",
       "      <td>As a BB you can kill in one shot if enemy has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>1</td>\n",
       "      <td>never a real competitor and probably was never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75200</th>\n",
       "      <td>0</td>\n",
       "      <td>thinking if we move to a hard system where it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15668</th>\n",
       "      <td>0</td>\n",
       "      <td>gt ; If you 're not clean-shaven but do n't sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170506</th>\n",
       "      <td>0</td>\n",
       "      <td>of there . Sure , shove a crazed violent dog o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175943</th>\n",
       "      <td>0</td>\n",
       "      <td>to save them that believe . * An* [ PC ] [ H ]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298340</th>\n",
       "      <td>0</td>\n",
       "      <td>would still expect free healthcare just for be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6754</th>\n",
       "      <td>1</td>\n",
       "      <td>. Wait , how do you invent a life raft ? Maybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266378</th>\n",
       "      <td>0</td>\n",
       "      <td>while remaining `` feeling good '' . That 's i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194700</th>\n",
       "      <td>0</td>\n",
       "      <td>if the game never existed we sadly would n't h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198683</th>\n",
       "      <td>0</td>\n",
       "      <td>Super easy to figure out where your from . Jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45095</th>\n",
       "      <td>0</td>\n",
       "      <td>n't have a go sign for those companies in thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95595</th>\n",
       "      <td>0</td>\n",
       "      <td>they would not be socialists . All of the ange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93071</th>\n",
       "      <td>0</td>\n",
       "      <td>warning about my pc being insecure with a 4x4 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273045</th>\n",
       "      <td>0</td>\n",
       "      <td>of the board , and honestly , it does it 's jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287443</th>\n",
       "      <td>0</td>\n",
       "      <td>fucking caused it . mine is 2x revenge/Endure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139900</th>\n",
       "      <td>0</td>\n",
       "      <td>, but in all honesty it was too strong and kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130607</th>\n",
       "      <td>0</td>\n",
       "      <td>... I 'm sorry , I could n't even finish that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57607</th>\n",
       "      <td>0</td>\n",
       "      <td>resubmit your server with a corrected title af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>0</td>\n",
       "      <td>from a tiny bit of a sore arm ( I got it in my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181185</th>\n",
       "      <td>0</td>\n",
       "      <td>. I actually had fun trying to get around both...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125592</th>\n",
       "      <td>0</td>\n",
       "      <td>be wrong but I saw the uavfutures review and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292851</th>\n",
       "      <td>0</td>\n",
       "      <td>toxic to dogs . Of course many websites will t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297302</th>\n",
       "      <td>0</td>\n",
       "      <td>. It is human , perhaps , to appreciate little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202117</th>\n",
       "      <td>0</td>\n",
       "      <td>need a group ... check out the LFG ( looking f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294344</th>\n",
       "      <td>0</td>\n",
       "      <td>I be able to fit ? Totally aggree , asians and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181734</th>\n",
       "      <td>0</td>\n",
       "      <td>] posts must be manually approved and we will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253097</th>\n",
       "      <td>0</td>\n",
       "      <td>setting as low as possible , including the low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119801</th>\n",
       "      <td>0</td>\n",
       "      <td>*one at a time* usually means women can learn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205403</th>\n",
       "      <td>0</td>\n",
       "      <td>is n't always the best place ) The Gigolo Aunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120461</th>\n",
       "      <td>0</td>\n",
       "      <td>refrain of vanguard insurrectionism , which se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156462</th>\n",
       "      <td>0</td>\n",
       "      <td>However as far as blood pressure goes , a stra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284179</th>\n",
       "      <td>0</td>\n",
       "      <td>. I just do n't feel like this one is worth it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237244</th>\n",
       "      <td>0</td>\n",
       "      <td>could and I 've been lucky to have quite a lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282844</th>\n",
       "      <td>0</td>\n",
       "      <td>road to travel down . It 's hard to find ED st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285711</th>\n",
       "      <td>0</td>\n",
       "      <td>do n't want to see people 's announcements ......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>0</td>\n",
       "      <td>Buy a car Om det nu är så att det är relativt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239545</th>\n",
       "      <td>0</td>\n",
       "      <td>Lord releases with minimal new content . And f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295117</th>\n",
       "      <td>0</td>\n",
       "      <td>[ removed ] Pet friendly , not shit , and affo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245390</th>\n",
       "      <td>0</td>\n",
       "      <td>right in that my career likely helped me analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259484</th>\n",
       "      <td>0</td>\n",
       "      <td>id=3732710 &amp; amp ; source=ff &amp; amp ; filetype=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25308</th>\n",
       "      <td>0</td>\n",
       "      <td>Seiko ] ( http : //imgur.com/gallery/LpABX ) I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77872</th>\n",
       "      <td>0</td>\n",
       "      <td>week with 2 exams last week , 1 four hour inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26730</th>\n",
       "      <td>0</td>\n",
       "      <td>my way just to let y'all know to chill tf out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66025</th>\n",
       "      <td>0</td>\n",
       "      <td>your best '' to follow the law is a pretty dar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261824</th>\n",
       "      <td>0</td>\n",
       "      <td>Why was Kimmi so intent on snatching the idol ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277402</th>\n",
       "      <td>0</td>\n",
       "      <td>I just found like one set of them in Will Powe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180438</th>\n",
       "      <td>0</td>\n",
       "      <td>like a gr What are the advantage with your too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99865</th>\n",
       "      <td>0</td>\n",
       "      <td>is something worth doing . 3 . You can make an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275914</th>\n",
       "      <td>0</td>\n",
       "      <td>it up so they could avoid the embarrassment of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88644</th>\n",
       "      <td>0</td>\n",
       "      <td>. They were not and theres really no correlati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27246</th>\n",
       "      <td>0</td>\n",
       "      <td>even after the restaurant mistake . If you man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        banned                                              words\n",
       "271582       0  its inaugural edition of the report 'Poverty a...\n",
       "72781        0  cool '' will suddenly make you unique and inte...\n",
       "197561       0  from him . If he is in your class and sits nex...\n",
       "70760        0  is now just a safe space nonsense zone when it...\n",
       "51467        0  is awesome ! Canada was there too . [ removed ...\n",
       "49464        0  Cyberface/Medic what if they win more than 8 g...\n",
       "144111       0  `` Stop Crossposting My Stuff ! '' ) ] ( https...\n",
       "249539       0  being built in Europe for whatever that 's wor...\n",
       "61102        0  As a BB you can kill in one shot if enemy has ...\n",
       "1953         1  never a real competitor and probably was never...\n",
       "75200        0  thinking if we move to a hard system where it ...\n",
       "15668        0  gt ; If you 're not clean-shaven but do n't sp...\n",
       "170506       0  of there . Sure , shove a crazed violent dog o...\n",
       "175943       0  to save them that believe . * An* [ PC ] [ H ]...\n",
       "298340       0  would still expect free healthcare just for be...\n",
       "6754         1  . Wait , how do you invent a life raft ? Maybe...\n",
       "266378       0  while remaining `` feeling good '' . That 's i...\n",
       "194700       0  if the game never existed we sadly would n't h...\n",
       "198683       0  Super easy to figure out where your from . Jus...\n",
       "45095        0  n't have a go sign for those companies in thei...\n",
       "95595        0  they would not be socialists . All of the ange...\n",
       "93071        0  warning about my pc being insecure with a 4x4 ...\n",
       "273045       0  of the board , and honestly , it does it 's jo...\n",
       "287443       0  fucking caused it . mine is 2x revenge/Endure ...\n",
       "139900       0  , but in all honesty it was too strong and kin...\n",
       "130607       0  ... I 'm sorry , I could n't even finish that ...\n",
       "57607        0  resubmit your server with a corrected title af...\n",
       "3648         0  from a tiny bit of a sore arm ( I got it in my...\n",
       "181185       0  . I actually had fun trying to get around both...\n",
       "125592       0  be wrong but I saw the uavfutures review and h...\n",
       "...        ...                                                ...\n",
       "292851       0  toxic to dogs . Of course many websites will t...\n",
       "297302       0  . It is human , perhaps , to appreciate little...\n",
       "202117       0  need a group ... check out the LFG ( looking f...\n",
       "294344       0  I be able to fit ? Totally aggree , asians and...\n",
       "181734       0  ] posts must be manually approved and we will ...\n",
       "253097       0  setting as low as possible , including the low...\n",
       "119801       0  *one at a time* usually means women can learn ...\n",
       "205403       0  is n't always the best place ) The Gigolo Aunt...\n",
       "120461       0  refrain of vanguard insurrectionism , which se...\n",
       "156462       0  However as far as blood pressure goes , a stra...\n",
       "284179       0  . I just do n't feel like this one is worth it...\n",
       "237244       0  could and I 've been lucky to have quite a lot...\n",
       "282844       0  road to travel down . It 's hard to find ED st...\n",
       "285711       0  do n't want to see people 's announcements ......\n",
       "4273         0  Buy a car Om det nu är så att det är relativt ...\n",
       "239545       0  Lord releases with minimal new content . And f...\n",
       "295117       0  [ removed ] Pet friendly , not shit , and affo...\n",
       "245390       0  right in that my career likely helped me analy...\n",
       "259484       0  id=3732710 & amp ; source=ff & amp ; filetype=...\n",
       "25308        0  Seiko ] ( http : //imgur.com/gallery/LpABX ) I...\n",
       "77872        0  week with 2 exams last week , 1 four hour inte...\n",
       "26730        0  my way just to let y'all know to chill tf out ...\n",
       "66025        0  your best '' to follow the law is a pretty dar...\n",
       "261824       0  Why was Kimmi so intent on snatching the idol ...\n",
       "277402       0  I just found like one set of them in Will Powe...\n",
       "180438       0  like a gr What are the advantage with your too...\n",
       "99865        0  is something worth doing . 3 . You can make an...\n",
       "275914       0  it up so they could avoid the embarrassment of...\n",
       "88644        0  . They were not and theres really no correlati...\n",
       "27246        0  even after the restaurant mistake . If you man...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = dfTrain.sample(frac=1)\n",
    "dfTrain.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303813572 words total, with a vocabulary size of 5364\n",
      "Max sentence length is 200\n",
      "Found 609126 unique tokens.\n",
      "(609127, 300)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "y_train = dfTrain[\"banned\"].values\n",
    "y_test = dfTest_balanced[\"banned\"].values\n",
    "X_train = dfTrain[\"words\"].values\n",
    "X_test = dfTest_balanced[\"words\"].values\n",
    "\n",
    "all_words = [word for tokens in X_train for word in tokens]\n",
    "all_sentence_lengths = [SEQ_LEN]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))\n",
    "\n",
    "\n",
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = len(ALL_VOCAB) # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = max(all_sentence_lengths) # max number of words in a comment to use\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(X_train.tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "\n",
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print(train_cnn_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 200, 300)          182738100 \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 198, 128)          115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_21 (MaxPooling (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 97, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 48, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 46, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 23, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 21, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               327936    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 183,329,461\n",
      "Trainable params: 183,329,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = len(train_word_index)+1\n",
    "TRAINABLE_EMBEDDINGS=True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS,\n",
    "          EMBEDDING_DIM,\n",
    "          weights=[train_embedding_weights],\n",
    "          input_length=MAX_SEQUENCE_LENGTH,\n",
    "          trainable=TRAINABLE_EMBEDDINGS))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(#loss='binary_crossentropy',\n",
    "              loss=[binary_focal_loss(alpha=.25, gamma=2)],\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',f1_m,precision_m, recall_m] )\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 307500 samples, validate on 3714 samples\n",
      "Epoch 1/100\n",
      "307500/307500 [==============================] - 36s 118us/step - loss: 3.2466 - acc: 0.9788 - f1_m: 0.2495 - precision_m: 0.7616 - recall_m: 0.1564 - val_loss: 41.1728 - val_acc: 0.5291 - val_f1_m: 0.1103 - val_precision_m: 1.0000 - val_recall_m: 0.0586\n",
      "Epoch 2/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 2.8532 - acc: 0.9802 - f1_m: 0.3304 - precision_m: 0.8350 - recall_m: 0.2178 - val_loss: 39.3790 - val_acc: 0.5355 - val_f1_m: 0.1329 - val_precision_m: 1.0000 - val_recall_m: 0.0713\n",
      "Epoch 3/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 2.6042 - acc: 0.9813 - f1_m: 0.3758 - precision_m: 0.8612 - recall_m: 0.2540 - val_loss: 33.8313 - val_acc: 0.5557 - val_f1_m: 0.2012 - val_precision_m: 0.9954 - val_recall_m: 0.1122\n",
      "Epoch 4/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 2.3511 - acc: 0.9822 - f1_m: 0.4317 - precision_m: 0.9017 - recall_m: 0.2980 - val_loss: 36.6569 - val_acc: 0.5468 - val_f1_m: 0.1717 - val_precision_m: 1.0000 - val_recall_m: 0.0941\n",
      "Epoch 5/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 2.0956 - acc: 0.9835 - f1_m: 0.4937 - precision_m: 0.9133 - recall_m: 0.3553 - val_loss: 38.9773 - val_acc: 0.5552 - val_f1_m: 0.1999 - val_precision_m: 0.9951 - val_recall_m: 0.1115\n",
      "Epoch 6/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 1.8981 - acc: 0.9849 - f1_m: 0.5567 - precision_m: 0.9255 - recall_m: 0.4180 - val_loss: 37.5902 - val_acc: 0.5991 - val_f1_m: 0.3341 - val_precision_m: 0.9850 - val_recall_m: 0.2020\n",
      "Epoch 7/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 1.6490 - acc: 0.9864 - f1_m: 0.6137 - precision_m: 0.9287 - recall_m: 0.4767 - val_loss: 35.1546 - val_acc: 0.5872 - val_f1_m: 0.3005 - val_precision_m: 0.9845 - val_recall_m: 0.1779\n",
      "Epoch 8/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 1.4925 - acc: 0.9878 - f1_m: 0.6679 - precision_m: 0.9398 - recall_m: 0.5387 - val_loss: 54.1931 - val_acc: 0.5687 - val_f1_m: 0.2437 - val_precision_m: 0.9874 - val_recall_m: 0.1396\n",
      "Epoch 9/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 1.2849 - acc: 0.9893 - f1_m: 0.7194 - precision_m: 0.9440 - recall_m: 0.6007 - val_loss: 73.5093 - val_acc: 0.5692 - val_f1_m: 0.2440 - val_precision_m: 0.9957 - val_recall_m: 0.1396\n",
      "Epoch 10/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 1.1463 - acc: 0.9906 - f1_m: 0.7603 - precision_m: 0.9511 - recall_m: 0.6533 - val_loss: 62.4542 - val_acc: 0.6012 - val_f1_m: 0.3404 - val_precision_m: 0.9838 - val_recall_m: 0.2064\n",
      "Epoch 11/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 1.0399 - acc: 0.9917 - f1_m: 0.7917 - precision_m: 0.9549 - recall_m: 0.6920 - val_loss: 64.9676 - val_acc: 0.6112 - val_f1_m: 0.3676 - val_precision_m: 0.9856 - val_recall_m: 0.2263\n",
      "Epoch 12/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 1.0122 - acc: 0.9918 - f1_m: 0.7948 - precision_m: 0.9556 - recall_m: 0.6983 - val_loss: 81.1815 - val_acc: 0.6142 - val_f1_m: 0.3763 - val_precision_m: 0.9813 - val_recall_m: 0.2334\n",
      "Epoch 13/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.8985 - acc: 0.9929 - f1_m: 0.8291 - precision_m: 0.9619 - recall_m: 0.7415 - val_loss: 69.8983 - val_acc: 0.6341 - val_f1_m: 0.4283 - val_precision_m: 0.9793 - val_recall_m: 0.2748\n",
      "Epoch 14/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.8000 - acc: 0.9935 - f1_m: 0.8423 - precision_m: 0.9587 - recall_m: 0.7652 - val_loss: 82.0391 - val_acc: 0.6217 - val_f1_m: 0.3955 - val_precision_m: 0.9849 - val_recall_m: 0.2481\n",
      "Epoch 15/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.7360 - acc: 0.9944 - f1_m: 0.8692 - precision_m: 0.9675 - recall_m: 0.7997 - val_loss: 66.7260 - val_acc: 0.6166 - val_f1_m: 0.3821 - val_precision_m: 0.9823 - val_recall_m: 0.2385\n",
      "Epoch 16/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.7160 - acc: 0.9946 - f1_m: 0.8723 - precision_m: 0.9692 - recall_m: 0.8039 - val_loss: 58.0728 - val_acc: 0.6327 - val_f1_m: 0.4248 - val_precision_m: 0.9811 - val_recall_m: 0.2717\n",
      "Epoch 17/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.7393 - acc: 0.9942 - f1_m: 0.8624 - precision_m: 0.9686 - recall_m: 0.7922 - val_loss: 99.1579 - val_acc: 0.6287 - val_f1_m: 0.4136 - val_precision_m: 0.9843 - val_recall_m: 0.2626\n",
      "Epoch 18/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.6492 - acc: 0.9949 - f1_m: 0.8828 - precision_m: 0.9706 - recall_m: 0.8205 - val_loss: 88.8043 - val_acc: 0.5891 - val_f1_m: 0.3040 - val_precision_m: 0.9948 - val_recall_m: 0.1801\n",
      "Epoch 19/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.6425 - acc: 0.9952 - f1_m: 0.8871 - precision_m: 0.9690 - recall_m: 0.8280 - val_loss: 77.8521 - val_acc: 0.6039 - val_f1_m: 0.3463 - val_precision_m: 0.9926 - val_recall_m: 0.2104\n",
      "Epoch 20/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.5945 - acc: 0.9955 - f1_m: 0.8943 - precision_m: 0.9729 - recall_m: 0.8381 - val_loss: 102.1722 - val_acc: 0.6195 - val_f1_m: 0.3892 - val_precision_m: 0.9862 - val_recall_m: 0.2433\n",
      "Epoch 21/100\n",
      "307500/307500 [==============================] - 35s 115us/step - loss: 0.5884 - acc: 0.9956 - f1_m: 0.8991 - precision_m: 0.9761 - recall_m: 0.8434 - val_loss: 100.3421 - val_acc: 0.6069 - val_f1_m: 0.3562 - val_precision_m: 0.9858 - val_recall_m: 0.2179\n",
      "Epoch 22/100\n",
      " 59904/307500 [====>.........................] - ETA: 28s - loss: 0.4936 - acc: 0.9964 - f1_m: 0.9192 - precision_m: 0.9787 - recall_m: 0.8753"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_cnn_data, y_train, epochs=100, batch_size=512,\n",
    "                       validation_data=(test_cnn_data, y_test),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

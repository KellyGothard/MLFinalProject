{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "SEED = 0\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Constants\n",
    "NB = 'naive bayes'\n",
    "SVM = 'SVM'\n",
    "RF = 'random forest'\n",
    "KNN = 'kNN'\n",
    "LG = \"logReg\"\n",
    "\n",
    "\n",
    "def make_models():\n",
    "    '''\n",
    "    Make a variety of model pipelines and parameter grids. Return a dict mapping from a model name\n",
    "    to a tuple of (model, param_grid). The param_grid is for use with GridSearchCV\n",
    "\n",
    "    :return: dict\n",
    "    '''\n",
    "\n",
    "    nb =  Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])\n",
    "\n",
    "    svm =  Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, shuffle=True,\n",
    "                              max_iter=10, n_jobs=-1\n",
    "                              # early_stopping=True, tol=1e-3, n_iter_no_change=5, validation_fraction=0.1\n",
    "                              ))\n",
    "    ])\n",
    "\n",
    "    knn =  Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=2, weights='uniform', p=1,  n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "    rf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', RandomForestClassifier(n_estimators=100, n_jobs=-1)),\n",
    "    ])\n",
    "    \n",
    "    logRg = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', LogisticRegression( n_jobs=-1)),\n",
    "    ])\n",
    "\n",
    "    return {LG:logRg, NB: nb, SVM: svm, RF: rf,  KNN: knn}\n",
    "\n",
    "\n",
    "def evaluate_model(y, pred):\n",
    "\n",
    "    print(classification_report(y, pred))\n",
    "    report = classification_report(y, pred, output_dict=True)\n",
    "    print('Confusion matrix: row is true class, col is predicted class')\n",
    "    cm = confusion_matrix(y, pred)\n",
    "    print(cm)\n",
    "    return report, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Data\n",
    "# TRAIN_DATA = \"../Data/Generated/RC_2016-10_Train.pkl\"\n",
    "# TEST_DATA = \"../Data/Generated/RC_2016-10_Test.pkl\"\n",
    "\n",
    "# postsTrain = pd.read_pickle(TRAIN_DATA)\n",
    "# postsTest = pd.read_pickle(TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test percent lost: 5.63\n",
      "Train percent lost: 0.03\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banned</th>\n",
       "      <th>words</th>\n",
       "      <th>split</th>\n",
       "      <th>word_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>440841</th>\n",
       "      <td>0</td>\n",
       "      <td>is related with vex ... suppose ) I saw a Sapp...</td>\n",
       "      <td>[is, related, with, vex, ..., suppose, ), I, s...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444289</th>\n",
       "      <td>0</td>\n",
       "      <td>end of the week , this week he was actually pu...</td>\n",
       "      <td>[end, of, the, week, ,, this, week, he, was, a...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436538</th>\n",
       "      <td>0</td>\n",
       "      <td>your credit score , however I 'm not sure if h...</td>\n",
       "      <td>[your, credit, score, ,, however, I, 'm, not, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721801</th>\n",
       "      <td>0</td>\n",
       "      <td>night but I bet the Steelers D did well . What...</td>\n",
       "      <td>[night, but, I, bet, the, Steelers, D, did, we...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545623</th>\n",
       "      <td>0</td>\n",
       "      <td>worst bit though is the sound effect implies t...</td>\n",
       "      <td>[worst, bit, though, is, the, sound, effect, i...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467945</th>\n",
       "      <td>0</td>\n",
       "      <td>and there 's a TON of discussion around the wh...</td>\n",
       "      <td>[and, there, 's, a, TON, of, discussion, aroun...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627647</th>\n",
       "      <td>0</td>\n",
       "      <td>to weekends for relaxing and having fun , holi...</td>\n",
       "      <td>[to, weekends, for, relaxing, and, having, fun...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361817</th>\n",
       "      <td>0</td>\n",
       "      <td>) , [ see IMGUR gallery with more pics here ] ...</td>\n",
       "      <td>[), ,, [, see, IMGUR, gallery, with, more, pic...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739472</th>\n",
       "      <td>0</td>\n",
       "      <td>385 . You increase this by maximizing your dam...</td>\n",
       "      <td>[385, ., You, increase, this, by, maximizing, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235344</th>\n",
       "      <td>0</td>\n",
       "      <td>and cracked app vectors . It 's still dangerou...</td>\n",
       "      <td>[and, cracked, app, vectors, ., It, 's, still,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        banned                                              words  \\\n",
       "440841       0  is related with vex ... suppose ) I saw a Sapp...   \n",
       "444289       0  end of the week , this week he was actually pu...   \n",
       "436538       0  your credit score , however I 'm not sure if h...   \n",
       "721801       0  night but I bet the Steelers D did well . What...   \n",
       "545623       0  worst bit though is the sound effect implies t...   \n",
       "467945       0  and there 's a TON of discussion around the wh...   \n",
       "627647       0  to weekends for relaxing and having fun , holi...   \n",
       "361817       0  ) , [ see IMGUR gallery with more pics here ] ...   \n",
       "739472       0  385 . You increase this by maximizing your dam...   \n",
       "235344       0  and cracked app vectors . It 's still dangerou...   \n",
       "\n",
       "                                                    split  word_cnt  \n",
       "440841  [is, related, with, vex, ..., suppose, ), I, s...       200  \n",
       "444289  [end, of, the, week, ,, this, week, he, was, a...       200  \n",
       "436538  [your, credit, score, ,, however, I, 'm, not, ...       200  \n",
       "721801  [night, but, I, bet, the, Steelers, D, did, we...       200  \n",
       "545623  [worst, bit, though, is, the, sound, effect, i...       200  \n",
       "467945  [and, there, 's, a, TON, of, discussion, aroun...       200  \n",
       "627647  [to, weekends, for, relaxing, and, having, fun...       200  \n",
       "361817  [), ,, [, see, IMGUR, gallery, with, more, pic...       200  \n",
       "739472  [385, ., You, increase, this, by, maximizing, ...       200  \n",
       "235344  [and, cracked, app, vectors, ., It, 's, still,...       200  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain_banned = pd.read_csv(\"../Data/Generated/200_words_10M_banned.csv\", delimiter=',')\n",
    "dftrain_banned.insert(0, \"banned\", 1)\n",
    "\n",
    "dftrain_notbanned = pd.read_csv(\"../Data/Generated/200_words_10M_notbanned.csv\", delimiter=',')\n",
    "dftrain_notbanned.insert(0, \"banned\", 0)\n",
    "\n",
    "dfTest = pd.read_csv(\"../Data/Generated/200_words_10M_test.csv\", delimiter=',')\n",
    "dfTest = dfTest.sample(frac=1)\n",
    "\n",
    "dfTest[\"split\"] = dfTest[\"words\"].map(lambda x: x.split(\" \"), na_action='ignore')\n",
    "dfTest[\"word_cnt\"] = dfTest[\"split\"].map(lambda x: len(x), na_action='ignore')\n",
    "print(\"Test percent lost: %.2f\" % (100*len(dfTest[dfTest[\"word_cnt\"] != 200])/ len(dfTest)))\n",
    "dfTest = dfTest[dfTest[\"word_cnt\"] == 200]\n",
    "\n",
    "dfTest_banned = dfTest[dfTest[\"banned\"]]\n",
    "dfTest_notbanned = dfTest[dfTest[\"banned\"] == False]\n",
    "\n",
    "TRAIN_BALANCE_RATIO = 100\n",
    "TEST_BALANCE_RATIO = 100\n",
    "TRAIN_N_COMMENTS = int(len(dftrain_banned)/1)\n",
    "TEST_N_COMMENTS = int(len(dfTest_banned)/2)\n",
    "\n",
    "dfTest_balanced = pd.concat([dfTest_banned.head(n=TEST_N_COMMENTS), dfTest_notbanned.head(n=TEST_BALANCE_RATIO*TEST_N_COMMENTS)]).sample(frac=1)\n",
    "\n",
    "dfTrain = pd.concat([dftrain_banned.head(n=TRAIN_N_COMMENTS), dftrain_notbanned.head(n=TRAIN_BALANCE_RATIO*TRAIN_N_COMMENTS)])\n",
    "\n",
    "dfTrain[\"split\"] = dfTrain[\"words\"].apply(lambda x: x.split(\" \"))\n",
    "dfTrain[\"word_cnt\"] = dfTrain[\"split\"].apply(lambda x: len(x))\n",
    "print(\"Train percent lost: %.2f\" % (100*len(dfTrain[dfTrain[\"word_cnt\"] != 200])/ len(dfTrain)))\n",
    "dfTrain = dfTrain[dfTrain[\"word_cnt\"]== 200]\n",
    "\n",
    "dfTrain = dfTrain.sample(frac=1)\n",
    "dfTrain.head(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 5\n",
    "\n",
    "models = make_models()\n",
    "\n",
    "results = {'kfold': kfold,\n",
    "           'trials': []}\n",
    "\n",
    "x_train = dfTrain[\"words\"].values\n",
    "y_train = dfTrain[\"banned\"].values\n",
    "\n",
    "x_test = dfTest[\"words\"].values\n",
    "y_test = dfTest[\"banned\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', RandomForestClassifier(n_estimators=1000, n_jobs=-1)),\n",
    "    ])\n",
    "rf.fit(x_train, y_train)\n",
    "pred_test = rf.predict(x_test)\n",
    "cr_train, cm_train = evaluate_model(y_test, pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "training and evaluating logReg\n",
      "dataset shapes: x_train: 44841, x_test: 339426, y_train: 44841, y_test: 339426\n",
      "train set evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/users/d/m/dmatthe1/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 32.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0a3c4c5d088e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcr_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=========================================================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    944\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for model_id in models:\n",
    "    result = {'model': model_id}\n",
    "    model = models[model_id]\n",
    "    original_model = copy.deepcopy(model)\n",
    "    print('==========================================================')\n",
    "    print(f'training and evaluating {model_id}')\n",
    "\n",
    "    print(f'dataset shapes: x_train: {len(x_train)}, x_test: {len(x_test)}, y_train: {len(y_train)}, y_test: {len(y_test)}')\n",
    "\n",
    "    print('train set evaluation')\n",
    "    model = copy.deepcopy(original_model)\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "        \n",
    "    pred_test = model.predict(x_test)\n",
    "    cr_train, cm_train = evaluate_model(y_test, pred_test)\n",
    "    print('==========================================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

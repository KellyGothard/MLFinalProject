{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Always import all needed libraries in the first cell\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import os\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Reshape\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model, CustomObjectScope\n",
    "import random\n",
    "\n",
    "np.random.seed(1) # this sets the seed so that the runs are consistent\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(clf,X,y,name):\n",
    "    print(name)\n",
    "    y_pred = cross_val_predict(clf, X, y, cv=10)\n",
    "    print(metrics.classification_report(y, y_pred))\n",
    "    conf = np.array(metrics.confusion_matrix(y, y_pred))\n",
    "    print(conf)\n",
    "    y_probas = clf.predict_proba(X)\n",
    "#     skplt.metrics.plot_roc_curve(y, y_probas, title=name+' ROC Curves', curves='each_class')\n",
    "    return metrics.f1_score(y,y_pred,pos_label=1, average='binary')\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test percent lost: 5.63\n"
     ]
    }
   ],
   "source": [
    "dftrain_banned = pd.read_csv(\"../Data/Generated/200_words_10M_banned.csv\", delimiter=',')\n",
    "dftrain_banned.insert(0, \"banned\", 1)\n",
    "\n",
    "dftrain_notbanned = pd.read_csv(\"../Data/Generated/200_words_10M_notbanned.csv\", delimiter=',')\n",
    "dftrain_notbanned.insert(0, \"banned\", 0)\n",
    "\n",
    "dfTest = pd.read_csv(\"../Data/Generated/200_words_10M_test.csv\", delimiter=',')\n",
    "dfTest = dfTest.sample(frac=1)\n",
    "\n",
    "dfTest[\"split\"] = dfTest[\"words\"].map(lambda x: x.split(\" \"), na_action='ignore')\n",
    "dfTest[\"word_cnt\"] = dfTest[\"split\"].map(lambda x: len(x), na_action='ignore')\n",
    "print(\"Test percent lost: %.2f\" % (100*len(dfTest[dfTest[\"word_cnt\"] != 200])/ len(dfTest)))\n",
    "dfTest = dfTest[dfTest[\"word_cnt\"] == 200]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006299239776679007\n",
      "0.004335934894269896\n",
      "0.006116000902730761\n"
     ]
    }
   ],
   "source": [
    "def group_by_subreddit(df):\n",
    "    subreddit_to_comments = {}\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"subreddit\"] in subreddit_to_comments:\n",
    "            subreddit_to_comments[row[\"subreddit\"]] += [row[\"words\"]]\n",
    "        else:\n",
    "            subreddit_to_comments[row[\"subreddit\"]] = [row[\"words\"]]\n",
    "    return subreddit_to_comments\n",
    "    \n",
    "random.seed(42)\n",
    "unique_subreddits = list(dfTest[\"subreddit\"].value_counts().keys())\n",
    "random.shuffle(unique_subreddits)\n",
    "SUBREDDIT_CNT = len(unique_subreddits)\n",
    "\n",
    "s1 = int (1/3 * SUBREDDIT_CNT)\n",
    "s2 = int (2/3 * SUBREDDIT_CNT)\n",
    "\n",
    "validation_test = unique_subreddits[:s1]\n",
    "\n",
    "threshold_test = unique_subreddits[s1:s2]\n",
    "\n",
    "testing_test = unique_subreddits[s2:]\n",
    "\n",
    "\n",
    "dfVal = dfTest[dfTest[\"subreddit\"].isin(validation_test)]\n",
    "print(dfVal[\"banned\"].sum()/ dfVal.shape[0])\n",
    "\n",
    "\n",
    "dfThresh = dfTest[dfTest[\"subreddit\"].isin(threshold_test)]\n",
    "print(dfThresh[\"banned\"].sum()/ dfThresh.shape[0])\n",
    "threshSubreddits  = group_by_subreddit(dfThresh)\n",
    "\n",
    "dfTestingTest = dfTest[dfTest[\"subreddit\"].isin(testing_test)]\n",
    "print(dfTestingTest[\"banned\"].sum()/ dfTestingTest.shape[0])\n",
    "testingTestSubreddits  = group_by_subreddit(dfTestingTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BALANCE_RATIO = 20\n",
    "TEST_BALANCE_RATIO = 185\n",
    "TRAIN_N_COMMENTS = int(len(dftrain_banned)/3)\n",
    "TEST_N_COMMENTS = int(len(dfTest_banned)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest_banned = dfVal[dfVal[\"banned\"]]\n",
    "dfTest_notbanned = dfVal[dfVal[\"banned\"] == False]\n",
    "\n",
    "dfTest_balanced = pd.concat([dfTest_banned.head(n=TEST_N_COMMENTS), dfTest_notbanned.head(n=TEST_BALANCE_RATIO*TEST_N_COMMENTS)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain_banned_tmp = pd.concat([dftrain_banned.head(n=TRAIN_N_COMMENTS)]*int(TRAIN_BALANCE_RATIO))\n",
    "dfTrain = pd.concat([dfTrain_banned_tmp, dftrain_notbanned.head(n=TRAIN_BALANCE_RATIO*TRAIN_N_COMMENTS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 2), (50000, 2))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain[dfTrain[\"banned\"]==1].shape, dfTrain[dfTrain[\"banned\"]==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train percent lost: 1.00\n"
     ]
    }
   ],
   "source": [
    "dfTrain[\"split\"] = dfTrain[\"words\"].apply(lambda x: x.split(\" \"))\n",
    "dfTrain[\"word_cnt\"] = dfTrain[\"split\"].apply(lambda x: len(x))\n",
    "print(\"Train percent lost: %.2f\" % (100*len(dfTrain[dfTrain[\"word_cnt\"] != 200])/ len(dfTrain)))\n",
    "dfTrain = dfTrain[dfTrain[\"word_cnt\"]== 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>banned</th>\n",
       "      <th>words</th>\n",
       "      <th>split</th>\n",
       "      <th>word_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34777</th>\n",
       "      <td>0</td>\n",
       "      <td>in some instances . There is a pretty famous c...</td>\n",
       "      <td>[in, some, instances, ., There, is, a, pretty,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38630</th>\n",
       "      <td>0</td>\n",
       "      <td>] f this suggestion of personhood is establish...</td>\n",
       "      <td>[], f, this, suggestion, of, personhood, is, e...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>1</td>\n",
       "      <td>bro to hang with . I met a few my last trip , ...</td>\n",
       "      <td>[bro, to, hang, with, ., I, met, a, few, my, l...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1</td>\n",
       "      <td>was the most popular answer to that query , so...</td>\n",
       "      <td>[was, the, most, popular, answer, to, that, qu...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14913</th>\n",
       "      <td>0</td>\n",
       "      <td>) ** but we know that 1+1=322 - &amp; gt ; equatio...</td>\n",
       "      <td>[), **, but, we, know, that, 1+1=322, -, &amp;, gt...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33164</th>\n",
       "      <td>0</td>\n",
       "      <td>thousands of HS like nothing ! Acknowledge my ...</td>\n",
       "      <td>[thousands, of, HS, like, nothing, !, Acknowle...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>1</td>\n",
       "      <td>for me at points . They do n't work if you did...</td>\n",
       "      <td>[for, me, at, points, ., They, do, n't, work, ...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>1</td>\n",
       "      <td>if you do n't yet have much to offer , put wha...</td>\n",
       "      <td>[if, you, do, n't, yet, have, much, to, offer,...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16252</th>\n",
       "      <td>0</td>\n",
       "      <td>I 've only listened Dan Carlin 's Hardcore His...</td>\n",
       "      <td>[I, 've, only, listened, Dan, Carlin, 's, Hard...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>1</td>\n",
       "      <td>and psychological warfare is a pretty valid po...</td>\n",
       "      <td>[and, psychological, warfare, is, a, pretty, v...</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       banned                                              words  \\\n",
       "34777       0  in some instances . There is a pretty famous c...   \n",
       "38630       0  ] f this suggestion of personhood is establish...   \n",
       "2305        1  bro to hang with . I met a few my last trip , ...   \n",
       "228         1  was the most popular answer to that query , so...   \n",
       "14913       0  ) ** but we know that 1+1=322 - & gt ; equatio...   \n",
       "33164       0  thousands of HS like nothing ! Acknowledge my ...   \n",
       "1677        1  for me at points . They do n't work if you did...   \n",
       "2072        1  if you do n't yet have much to offer , put wha...   \n",
       "16252       0  I 've only listened Dan Carlin 's Hardcore His...   \n",
       "692         1  and psychological warfare is a pretty valid po...   \n",
       "\n",
       "                                                   split  word_cnt  \n",
       "34777  [in, some, instances, ., There, is, a, pretty,...       200  \n",
       "38630  [], f, this, suggestion, of, personhood, is, e...       200  \n",
       "2305   [bro, to, hang, with, ., I, met, a, few, my, l...       200  \n",
       "228    [was, the, most, popular, answer, to, that, qu...       200  \n",
       "14913  [), **, but, we, know, that, 1+1=322, -, &, gt...       200  \n",
       "33164  [thousands, of, HS, like, nothing, !, Acknowle...       200  \n",
       "1677   [for, me, at, points, ., They, do, n't, work, ...       200  \n",
       "2072   [if, you, do, n't, yet, have, much, to, offer,...       200  \n",
       "16252  [I, 've, only, listened, Dan, Carlin, 's, Hard...       200  \n",
       "692    [and, psychological, warfare, is, a, pretty, v...       200  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = dfTrain.sample(frac=1)\n",
    "dfTrain.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(dfTrain[\"words\"])\n",
    "y_train = dfTrain[\"banned\"]\n",
    "\n",
    "X_test = vectorizer.transform(dfTest_balanced[\"words\"])\n",
    "y_test = dfTest_balanced[\"banned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98995, 193952)\n",
      "193952\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]  # Number of features\n",
    "print(X_train.shape)\n",
    "print(input_dim)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
    "# model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dense_6_input (InputLayer)      (None, 193952)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 193952)       0           dense_6_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 1)            19416521    lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Concatenate)          (None, 1)            0           sequential_2[1][0]               \n",
      "                                                                 sequential_2[2][0]               \n",
      "                                                                 sequential_2[3][0]               \n",
      "                                                                 sequential_2[4][0]               \n",
      "                                                                 sequential_2[5][0]               \n",
      "                                                                 sequential_2[6][0]               \n",
      "                                                                 sequential_2[7][0]               \n",
      "                                                                 sequential_2[8][0]               \n",
      "==================================================================================================\n",
      "Total params: 19,416,521\n",
      "Trainable params: 19,416,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "parallel_model = multi_gpu_model(model, gpus=8, cpu_merge=False)\n",
    "parallel_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',f1_m,precision_m, recall_m])\n",
    "parallel_model.summary()\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['acc',f1_m,precision_m, recall_m])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98995 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "98995/98995 [==============================] - 145s 1ms/step - loss: 0.2650 - acc: 0.8764 - f1_m: 0.7945 - precision_m: 0.9613 - recall_m: 0.7653 - val_loss: 0.0439 - val_acc: 0.9907 - val_f1_m: 0.2823 - val_precision_m: 0.2540 - val_recall_m: 0.3742\n",
      "Epoch 2/2\n",
      "98995/98995 [==============================] - 140s 1ms/step - loss: 0.0068 - acc: 0.9988 - f1_m: 0.9987 - precision_m: 0.9979 - recall_m: 0.9996 - val_loss: 0.0515 - val_acc: 0.9911 - val_f1_m: 0.3076 - val_precision_m: 0.2823 - val_recall_m: 0.4510\n"
     ]
    }
   ],
   "source": [
    "history = parallel_model.fit(X_train, y_train,\n",
    "                    epochs=2,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test[:10000], y_test[:10000]),\n",
    "                    batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model.save(\"../Data/Cached/Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CustomObjectScope({\"f1_m\": f1_m, \"cross_val\":cross_val, \"recall_m\":recall_m, \"precision_m\":precision_m}):\n",
    "    model_loaded = load_model(\"../Data/Cached/Model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_thresh = vectorizer.transform(flatten(threshSubreddits.values()))\n",
    "\n",
    "def predict_batch(examples):\n",
    "    X_thresh = vectorizer.transform(examples)\n",
    "    return parallel_model.predict(X_thresh, batch_size=256)\n",
    "    \n",
    "    \n",
    "predictions = Parallel(n_jobs=-1)(delayed(predict_batch)(examples) for examples in tqdm(threshSubreddits.values()))\n",
    "\n",
    "# for subreddit, examples in threshSubreddits.items():\n",
    "#     X_thresh = vectorizer.transform(examples)\n",
    "#     print(len(examples), flush=True)\n",
    "#     thresh_predications[subreddit] = parallel_model.predict(X_thresh, batch_size=len(examples))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() got an unexpected keyword argument 'use_multiprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-7b52c6a30bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() got an unexpected keyword argument 'use_multiprocessing'"
     ]
    }
   ],
   "source": [
    "prediction = model_loaded.predict(X_thresh, use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
